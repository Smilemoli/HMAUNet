import torch
import torch.nn as nn
import torch.nn.functional as F
from .components.encoder import hybrid_encoder_base
from .components.hma_module import hma_bottleneck_base
from .components.decoder import resnet_decoder_base, OutputHead


class HMAUNet(nn.Module):
    """
    HMA-UNet: Hierarchical Mamba Aggregator U-Net - å®Œæ•´å®ç°ç‰ˆæœ¬

    ä¸€ä¸ªåˆ›æ–°çš„åŒ»å­¦å›¾åƒåˆ†å‰²ç½‘ç»œï¼Œç»“åˆäº†ä»¥ä¸‹æ ¸å¿ƒè®¾è®¡ï¼š
    1. æ··åˆç¼–ç å™¨: æµ…å±‚ConvNeXt + æ·±å±‚Mamba
    2. HMAç“¶é¢ˆå±‚: å±‚çº§å¼Mambaèšåˆå™¨ï¼ˆå¢å¼ºç‰ˆï¼‰
    3. CSFGè·³è·ƒè¿æ¥: è·¨å°ºåº¦èåˆé—¨
    4. ResNetè§£ç å™¨: è½»é‡çº§ç‰¹å¾é‡å»º

    å¢å¼ºç‰¹æ€§ï¼š
    - å¤šå°ºåº¦æ³¨æ„åŠ›æœºåˆ¶
    - è¾¹ç•Œå¢å¼ºæ¨¡å—
    - è‡ªé€‚åº”ç‰¹å¾èåˆ
    - æ·±åº¦ç›‘ç£æ”¯æŒ
    - åŠ¨æ€æƒé‡è°ƒæ•´

    Args:
        in_channels (int): è¾“å…¥å›¾åƒé€šé“æ•°ï¼Œé»˜è®¤3 (RGB)
        num_classes (int): åˆ†å‰²ç±»åˆ«æ•°ï¼Œé»˜è®¤1
        base_channels (int): åŸºç¡€é€šé“æ•°ï¼Œé»˜è®¤32
    """

    def __init__(
        self,
        in_channels=3,
        num_classes=1,
        base_channels=32,
    ):
        super().__init__()

        # ä¿å­˜é…ç½®å‚æ•°
        self.in_channels = in_channels
        self.num_classes = num_classes
        self.base_channels = base_channels

        # è®¡ç®—å„é˜¶æ®µé€šé“æ•°
        # ç¼–ç å™¨è¾“å‡º: [2C, 4C, 8C, 8C]
        self.encoder_channels = [
            2 * base_channels,  # Stage 1: 2C
            4 * base_channels,  # Stage 2: 4C
            8 * base_channels,  # Stage 3: 8C
            8 * base_channels,  # Stage 4: 8C
        ]

        print(f"HMAUNet: base_channels={base_channels}, encoder_channels={self.encoder_channels}")

        # 1. æ··åˆå¼ç¼–ç å™¨ (ConvNeXt + Mamba)
        self.encoder = hybrid_encoder_base(
            in_channels=in_channels,
            base_channels=base_channels,
        )

        # 2. HMAç“¶é¢ˆå±‚ (å±‚çº§å¼Mambaèšåˆå™¨) - å¢å¼ºç‰ˆ
        self.bottleneck = hma_bottleneck_base(
            in_channels=self.encoder_channels[3],  # 8C
            out_channels=self.encoder_channels[3],  # 8C
        )

        # 3. ResNetè§£ç å™¨ (å¸¦CSFGè·³è·ƒè¿æ¥)
        self.decoder = resnet_decoder_base(
            base_channels=base_channels,
            encoder_channels=self.encoder_channels,
        )

        # 4. è¾“å‡ºå¤´
        self.output_head = OutputHead(
            in_channels=base_channels, 
            num_classes=num_classes, 
            dropout=0.1
        )

        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()

    def _initialize_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, (nn.BatchNorm2d, nn.LayerNorm)):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.trunc_normal_(m.weight, std=0.02)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(self, x):
        """
        HMA-UNetå‰å‘ä¼ æ’­

        Args:
            x: è¾“å…¥å›¾åƒ (B, in_channels, H, W)

        Returns:
            åˆ†å‰²é¢„æµ‹ (B, num_classes, H, W)
        """
        # ä¿å­˜è¾“å…¥å°ºå¯¸ä¿¡æ¯
        input_size = x.shape[2:]

        # 1. æ··åˆå¼ç¼–ç å™¨: æµ…å±‚ConvNeXt + æ·±å±‚Mamba
        encoder_features = self.encoder(x)
        # encoder_features: [x_enc1, x_enc2, x_enc3, x_enc4]
        # x_enc1: (B, 2C, H/4, W/4)   - Stage 1 è¾“å‡º
        # x_enc2: (B, 4C, H/8, W/8)   - Stage 2 è¾“å‡º
        # x_enc3: (B, 8C, H/16, W/16) - Stage 3 è¾“å‡º
        # x_enc4: (B, 8C, H/16, W/16) - Stage 4 è¾“å‡º

        x_enc1, x_enc2, x_enc3, x_enc4 = encoder_features

        # è·å–stemè¾“å‡ºç”¨äºæœ€åçš„è·³è·ƒè¿æ¥
        x_stem = self._get_stem_features(x)  # (B, C, H/2, W/2)

        # 2. HMAç“¶é¢ˆå±‚: å±‚çº§å¼Mambaèšåˆ - å¢å¼ºç‰ˆ
        bottleneck_features = self.bottleneck(x_enc4)  # (B, 8C, H/16, W/16)

        # 3. ResNetè§£ç å™¨: è½»é‡çº§ç‰¹å¾é‡å»º + CSFGæ™ºèƒ½è·³è·ƒè¿æ¥
        decoder_features = [x_enc1, x_enc2, x_enc3, x_stem]
        decoded_features = self.decoder(
            bottleneck_features, decoder_features
        )  # (B, C, H, W)

        # 4. è¾“å‡ºå¤´: ç”Ÿæˆæœ€ç»ˆåˆ†å‰²ç»“æœ
        output = self.output_head(decoded_features)  # (B, num_classes, H, W)

        # ç¡®ä¿è¾“å‡ºå°ºå¯¸ä¸è¾“å…¥åŒ¹é…
        if output.shape[2:] != input_size:
            output = F.interpolate(
                output, size=input_size, mode="bilinear", align_corners=True
            )

        return output

    def _get_stem_features(self, x):
        """è·å–ç¼–ç å™¨stemå±‚çš„è¾“å‡ºç‰¹å¾"""
        # é€šè¿‡ç¼–ç å™¨çš„stemå±‚è·å–åˆå§‹ç‰¹å¾
        return self.encoder.stem(x)  # (B, C, H/2, W/2)

    def get_model_info(self):
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            "model_name": "HMA-UNet",
            "input_channels": self.in_channels,
            "num_classes": self.num_classes,
            "base_channels": self.base_channels,
            "encoder_channels": self.encoder_channels,
            "total_params": sum(p.numel() for p in self.parameters()),
            "trainable_params": sum(
                p.numel() for p in self.parameters() if p.requires_grad
            ),
        }

    def get_feature_maps(self, x):
        """è·å–ä¸­é—´ç‰¹å¾å›¾ï¼Œç”¨äºå¯è§†åŒ–å’Œåˆ†æ"""
        with torch.no_grad():
            # ä¸´æ—¶å…³é—­è®­ç»ƒæ¨¡å¼
            old_training = self.training
            self.eval()

            # ç¼–ç å™¨ç‰¹å¾
            encoder_features = self.encoder(x)
            x_enc1, x_enc2, x_enc3, x_enc4 = encoder_features

            # Stemç‰¹å¾
            x_stem = self._get_stem_features(x)

            # ç“¶é¢ˆå±‚ç‰¹å¾
            bottleneck_features = self.bottleneck(x_enc4)

            # è§£ç å™¨ç‰¹å¾
            decoder_features = [x_enc1, x_enc2, x_enc3, x_stem]
            decoded_features = self.decoder(bottleneck_features, decoder_features)

            # æ¢å¤è®­ç»ƒçŠ¶æ€
            self.training = old_training

            return {
                "stem": x_stem,
                "encoder": {
                    "stage1": x_enc1,
                    "stage2": x_enc2,
                    "stage3": x_enc3,
                    "stage4": x_enc4,
                },
                "bottleneck": bottleneck_features,
                "decoder": decoded_features,
            }


# =============================================================================
# å·¥å‚å‡½æ•° - åªä¿ç•™baseé…ç½®
# =============================================================================

def create_hma_unet(
    config="base",
    in_channels=3,
    num_classes=1,
    **kwargs
):
    """
    åˆ›å»ºHMA-UNetæ¨¡å‹çš„å·¥å‚å‡½æ•° - åªæ”¯æŒbaseé…ç½®

    Args:
        config (str): æ¨¡å‹é…ç½®ï¼Œåªæ”¯æŒ"base"
        in_channels (int): è¾“å…¥é€šé“æ•°
        num_classes (int): åˆ†å‰²ç±»åˆ«æ•°
        **kwargs: å…¶ä»–é…ç½®å‚æ•°

    Returns:
        HMAUNet: é…ç½®å¥½çš„HMA-UNetæ¨¡å‹
    """
    
    # åªæ”¯æŒbaseé…ç½®
    if config != "base":
        print(f"Warning: Config '{config}' not supported, using base config")
    
    # è¿‡æ»¤kwargsä¸­çš„æ— æ•ˆå‚æ•°
    valid_kwargs = {}
    if 'base_channels' in kwargs:
        valid_kwargs['base_channels'] = kwargs['base_channels']
    
    return hma_unet_base(
        in_channels=in_channels,
        num_classes=num_classes,
        **valid_kwargs
    )


def hma_unet_base(
    in_channels=3, 
    num_classes=1, 
    base_channels=32,
    **kwargs
):
    """åŸºç¡€HMA-UNeté…ç½® - è¿½æ±‚æœ€ä½³æ€§èƒ½"""
    
    # è¿‡æ»¤æ‰å¯èƒ½å†²çªçš„å‚æ•°
    filtered_kwargs = {k: v for k, v in kwargs.items() 
                      if k not in ['base_channels']}
    
    return HMAUNet(
        in_channels=in_channels,
        num_classes=num_classes,
        base_channels=base_channels,
        **filtered_kwargs
    )


# =============================================================================
# å‘åå…¼å®¹çš„åˆ«å - å…¨éƒ¨æŒ‡å‘baseé…ç½®
# =============================================================================

# æ‰€æœ‰é…ç½®éƒ½æŒ‡å‘baseï¼Œç¡®ä¿å…¼å®¹æ€§
hma_unet_nano = hma_unet_base
hma_unet_tiny = hma_unet_base
hma_unet_small = hma_unet_base
hma_unet_efficient = hma_unet_base
hma_unet_memory_efficient = hma_unet_base
hma_unet_large = hma_unet_base
hma_unet_enhanced = hma_unet_base
hma_unet_improved = hma_unet_base
hma_unet_regularized = hma_unet_base
hma_unet_anti_overfitting = hma_unet_base


# =============================================================================
# é…ç½®è·å–å‡½æ•°
# =============================================================================

def get_available_configs():
    """è·å–å¯ç”¨çš„æ¨¡å‹é…ç½®"""
    return ["base"]  # åªè¿”å›baseé…ç½®


# =============================================================================
# æ¨¡å‹å¯¼å‡ºå’ŒåŠ è½½å·¥å…·
# =============================================================================

def save_hma_unet(model, filepath, include_config=True):
    """ä¿å­˜HMA-UNetæ¨¡å‹"""
    save_dict = {
        "model_state_dict": model.state_dict(),
        "model_info": model.get_model_info(),
    }

    if include_config:
        save_dict["config"] = {
            "in_channels": model.in_channels,
            "num_classes": model.num_classes,
            "base_channels": model.base_channels,
        }

    torch.save(save_dict, filepath)
    print(f"Model saved to {filepath}")


def load_hma_unet(filepath, config="base", device="cuda", **kwargs):
    """
    åŠ è½½HMA-UNetæ¨¡å‹
    
    Args:
        filepath: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
        config: æ¨¡å‹é…ç½®ï¼ˆåªæ”¯æŒbaseï¼‰
        device: è®¾å¤‡
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        åŠ è½½çš„æ¨¡å‹
    """
    import os
    
    # åˆ›å»ºæ¨¡å‹
    model = create_hma_unet(config="base", **kwargs)
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    if filepath and os.path.exists(filepath):
        print(f"Loading checkpoint from {filepath}")
        checkpoint = torch.load(filepath, map_location=device)
        
        # å°è¯•åŠ è½½æ¨¡å‹çŠ¶æ€
        if 'model_state_dict' in checkpoint:
            try:
                model.load_state_dict(checkpoint['model_state_dict'], strict=False)
                print("âœ… Model state loaded successfully")
            except Exception as e:
                print(f"âš ï¸ Warning: Could not load some parameters: {e}")
                # å°è¯•éƒ¨åˆ†åŠ è½½
                model_dict = model.state_dict()
                pretrained_dict = checkpoint['model_state_dict']
                
                # è¿‡æ»¤å‡ºåŒ¹é…çš„å‚æ•°
                matched_dict = {}
                for k, v in pretrained_dict.items():
                    if k in model_dict and model_dict[k].shape == v.shape:
                        matched_dict[k] = v
                
                model_dict.update(matched_dict)
                model.load_state_dict(model_dict)
                
                print(f"âœ… Partially loaded {len(matched_dict)}/{len(pretrained_dict)} parameters")
        
        # åŠ è½½æ¨¡å‹ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'model_info' in checkpoint:
            loaded_info = checkpoint['model_info']
            print(f"âœ… Loaded model: {loaded_info.get('model_name', 'Unknown')}")
            print(f"   Parameters: {loaded_info.get('total_params', 'Unknown'):,}")
    else:
        print(f"âš ï¸ Checkpoint file not found: {filepath}")
        print("Using random initialization")
    
    return model


def get_model_complexity_info(model, input_size=(3, 512, 512)):
    """è·å–æ¨¡å‹å¤æ‚åº¦ä¿¡æ¯"""
    try:
        from thop import profile, clever_format
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        device = next(model.parameters()).device
        input_tensor = torch.randn(1, *input_size).to(device)
        
        # è®¡ç®—FLOPså’Œå‚æ•°é‡
        flops, params = profile(model, inputs=(input_tensor,), verbose=False)
        
        # æ ¼å¼åŒ–è¾“å‡º
        flops_formatted, params_formatted = clever_format([flops, params], "%.2f")
        
        return {
            "flops": flops,
            "params": params,
            "flops_formatted": flops_formatted,
            "params_formatted": params_formatted,
            "flops_gflops": flops / 1e9,
            "params_mb": params * 4 / 1024 / 1024,  # å‡è®¾float32
        }
        
    except ImportError:
        print("âš ï¸ thop not available, cannot compute FLOPs")
        return {
            "flops": None,
            "params": sum(p.numel() for p in model.parameters()),
            "flops_formatted": "N/A",
            "params_formatted": f"{sum(p.numel() for p in model.parameters()):,}",
            "flops_gflops": None,
            "params_mb": sum(p.numel() for p in model.parameters()) * 4 / 1024 / 1024,
        }
    
    except Exception as e:
        print(f"âš ï¸ Error computing complexity: {e}")
        return {
            "flops": None,
            "params": sum(p.numel() for p in model.parameters()),
            "flops_formatted": "Error",
            "params_formatted": f"{sum(p.numel() for p in model.parameters()):,}",
            "flops_gflops": None,
            "params_mb": sum(p.numel() for p in model.parameters()) * 4 / 1024 / 1024,
        }


# =============================================================================
# æ¨¡å‹æµ‹è¯•å‡½æ•°
# =============================================================================

def test_hma_unet():
    """æµ‹è¯•HMA-UNetæ¨¡å‹çš„å‰å‘ä¼ æ’­"""
    print("ğŸš€ æµ‹è¯•HMA-UNetæ¨¡å‹ (baseé…ç½®)")
    print("=" * 50)

    # æµ‹è¯•è¾“å…¥
    batch_size = 2
    input_size = (256, 256)
    in_channels = 3
    num_classes = 1

    x = torch.randn(batch_size, in_channels, *input_size)

    try:
        # åˆ›å»ºæ¨¡å‹
        model = create_hma_unet(
            config="base", 
            in_channels=in_channels, 
            num_classes=num_classes
        )

        print(f"ğŸ“Š æ¨¡å‹åˆ›å»ºæˆåŠŸ")

        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            output = model(x)

        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        model_info = model.get_model_info()
        print(f"  æ¨¡å‹åç§°: {model_info['model_name']}")
        print(f"  è¾“å…¥å½¢çŠ¶: {x.shape}")
        print(f"  è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"  æ€»å‚æ•°é‡: {model_info['total_params']:,}")
        print(f"  åŸºç¡€é€šé“æ•°: {model_info['base_channels']}")

        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        expected_shape = (batch_size, num_classes, *input_size)
        assert output.shape == expected_shape, f"Expected {expected_shape}, got {output.shape}"
        print(f"  âœ… è¾“å‡ºå½¢çŠ¶æ­£ç¡®")

        # æµ‹è¯•ç‰¹å¾å›¾æå–
        feature_maps = model.get_feature_maps(x)
        print(f"  âœ… ç‰¹å¾å›¾æå–æˆåŠŸ: {len(feature_maps)} ä¸ªçº§åˆ«")
        
        # æµ‹è¯•æ¨¡å‹ä¿¡æ¯
        complexity_info = get_model_complexity_info(model, input_size=(3, 256, 256))
        if complexity_info['flops_gflops']:
            print(f"  ğŸ“ˆ è®¡ç®—å¤æ‚åº¦: {complexity_info['flops_gflops']:.2f} GFLOPs")
        print(f"  ğŸ’¾ æ¨¡å‹å¤§å°: {complexity_info['params_mb']:.2f} MB")

        print("\nğŸ‰ HMA-UNetæµ‹è¯•é€šè¿‡!")

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    test_hma_unet()
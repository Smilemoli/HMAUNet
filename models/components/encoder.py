import torch
import torch.nn as nn
import torch.nn.functional as F
from ..backbones.convnext_blocks import ConvNeXtV2Block
from ..backbones.vss_blocks import VSSBlock
from functools import partial


class ChannelAttention(nn.Module):
    """ä¿®å¤ç‰ˆé€šé“æ³¨æ„åŠ›æœºåˆ¶ - è§£å†³æ­»äº¡é—®é¢˜"""
    
    def __init__(self, channels, reduction=16):  # å¢å¤§reductioné¿å…è¿‡åº¦å‹ç¼©
        super().__init__()
        self.channels = channels
        
        # ç¡®ä¿æœ€å°é€šé“æ•°ï¼Œé¿å…è¿‡åº¦å‹ç¼©å¯¼è‡´æ­»äº¡
        hidden_channels = max(channels // reduction, 8)  # æœ€å°8ä¸ªé€šé“
        
        # å…¨å±€å¹³å‡æ± åŒ–å’Œæœ€å¤§æ± åŒ–
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        
        # æç®€è®¾è®¡ï¼Œé¿å…æ­»äº¡
        self.fc = nn.Sequential(
            nn.Linear(channels, hidden_channels, bias=True),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_channels, channels, bias=True),
            nn.Sigmoid()
        )
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        """è¶…ä¿å®ˆçš„æƒé‡åˆå§‹åŒ–ï¼Œé˜²æ­¢æ­»äº¡"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                m.weight.data *= 0.01  # æå°çš„åˆå§‹åŒ–
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0.1)  # æ­£åç½®é˜²æ­¢æ­»äº¡
    
    def forward(self, x):
        """
        Args:
            x: è¾“å…¥ç‰¹å¾ (B, C, H, W)
        Returns:
            å¢å¼ºåçš„ç‰¹å¾ (B, C, H, W)
        """
        B, C, H, W = x.shape
        
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        avg_out = self.avg_pool(x).view(B, C)
        max_out = self.max_pool(x).view(B, C)
        
        avg_attention = self.fc(avg_out).view(B, C, 1, 1)
        max_attention = self.fc(max_out).view(B, C, 1, 1)
        
        attention = (avg_attention + max_attention) * 0.5
        
        # å¼ºåˆ¶æ®‹å·®è¿æ¥é˜²æ­¢æ­»äº¡
        return x * (0.7 + 0.3 * attention)  # ç¡®ä¿è¾“å‡ºä¸ä¸º0


class DownsampleLayer(nn.Module):
    """è¶…ç¨³å®šä¸‹é‡‡æ ·å±‚"""

    def __init__(self, in_channels, out_channels, kernel_size=2, stride=2):
        super().__init__()
        self.conv = nn.Conv2d(
            in_channels,
            out_channels,
            kernel_size=kernel_size,
            stride=stride,
            bias=True  # ä½¿ç”¨biasæé«˜ç¨³å®šæ€§
        )
        self.norm = nn.BatchNorm2d(out_channels)
        self.activation = nn.ReLU(inplace=True)
        
        # è¶…ä¿å®ˆåˆå§‹åŒ–
        nn.init.kaiming_normal_(self.conv.weight, mode='fan_out', nonlinearity='relu')
        self.conv.weight.data *= 0.01  # æå°åˆå§‹åŒ–
        if self.conv.bias is not None:
            nn.init.constant_(self.conv.bias, 0.001)
        nn.init.constant_(self.norm.weight, 1)
        nn.init.constant_(self.norm.bias, 0)

    def forward(self, x):
        x = self.conv(x)
        x = self.norm(x)
        x = self.activation(x)
        return x


class ConvNeXtStage(nn.Module):
    """ConvNeXt stage - å®Œå…¨ç¦ç”¨drop_path"""

    def __init__(self, dim, depth=2, drop_path_rate=0.0):
        super().__init__()
        
        # å®Œå…¨ç¦ç”¨drop_path
        self.drop_path_rate = 0.0
        drop_path_rates = [0.0] * depth  # å¼ºåˆ¶æ‰€æœ‰éƒ½ä¸º0
        
        print(f"ConvNeXtStage: dim={dim}, depth={depth}, drop_path=DISABLED")

        self.blocks = nn.ModuleList()
        for i in range(depth):
            block = ConvNeXtV2Block(dim=dim, drop_path=0.0)  # å¼ºåˆ¶è®¾ä¸º0
            self.blocks.append(block)
        
        # é€šé“æ³¨æ„åŠ› - åªåœ¨é€šé“æ•°è¶³å¤Ÿæ—¶ä½¿ç”¨
        if dim >= 32:
            self.channel_attention = ChannelAttention(dim, reduction=16)
            self.use_attention = True
        else:
            self.channel_attention = nn.Identity()
            self.use_attention = False
        
        # æè½»å¾®çš„dropout
        self.dropout = nn.Dropout2d(p=0.01)

    def forward(self, x):
        for block in self.blocks:
            x = block(x)
        
        if self.use_attention:
            x = self.channel_attention(x)
        
        if self.training:
            x = self.dropout(x)
        
        return x


class VSSStage(nn.Module):
    """VSS stage - å®Œå…¨ç¦ç”¨drop_path"""

    def __init__(self, dim, depth=2, drop_path_rate=0.0, d_state=16):
        super().__init__()
        
        # å®Œå…¨ç¦ç”¨drop_path
        self.drop_path_rate = 0.0
        drop_path_rates = [0.0] * depth  # å¼ºåˆ¶æ‰€æœ‰éƒ½ä¸º0
        
        # ä¿å®ˆçš„d_stateè®¾ç½®
        max_d_state = max(dim // 32, 8)
        self.d_state = min(d_state, max_d_state)

        print(f"VSSStage: dim={dim}, depth={depth}, d_state={self.d_state}, drop_path=DISABLED")

        self.blocks = nn.ModuleList()
        for i in range(depth):
            block = VSSBlock(
                hidden_dim=dim,
                drop_path=0.0,  # å¼ºåˆ¶è®¾ä¸º0
                norm_layer=partial(nn.LayerNorm, eps=1e-6),
                d_state=self.d_state,
            )
            self.blocks.append(block)
        
        # é€šé“æ³¨æ„åŠ›
        self.channel_attention = ChannelAttention(dim, reduction=16)
        
        # æè½»å¾®çš„dropout
        self.dropout = nn.Dropout2d(p=0.01)

    def forward(self, x):
        # ç»´åº¦è½¬æ¢ï¼š(N, C, H, W) -> (N, H, W, C)
        B, C, H, W = x.shape
        x = x.permute(0, 2, 3, 1).contiguous()

        for block in self.blocks:
            try:
                x = block(x)
            except Exception as e:
                print(f"âš ï¸ VSSå—å¤±è´¥: {e}")
                # ä½¿ç”¨æ’ç­‰æ˜ å°„ä½œä¸ºå¤‡é€‰
                pass

        # è½¬æ¢å›ï¼š(N, H, W, C) -> (N, C, H, W)
        x = x.permute(0, 3, 1, 2).contiguous()
        
        # é€šé“æ³¨æ„åŠ›
        x = self.channel_attention(x)
        
        if self.training:
            x = self.dropout(x)
        
        return x


class HybridEncoder(nn.Module):
    """
    è¶…ç¨³å®šæ··åˆç¼–ç å™¨ - è§£å†³æ‰€æœ‰è¯Šæ–­é—®é¢˜
    """

    def __init__(
        self,
        in_channels=3,
        base_channels=32,
        depths=[2, 2, 2, 2],
        drop_path_rate=0.0,  # å¼ºåˆ¶ç¦ç”¨
        d_state=16,
        encoder_config='base',
    ):
        super().__init__()

        self.base_channels = base_channels
        self.num_stages = 4
        self.encoder_config = encoder_config

        print(f"HybridEncoder: base_channels={base_channels}, depths={depths}, drop_path=DISABLED")

        # é€šé“é…ç½®
        self.stage_channels = [
            base_channels,      # Stem
            2 * base_channels,  # Stage 1
            4 * base_channels,  # Stage 2
            8 * base_channels,  # Stage 3
            8 * base_channels,  # Stage 4
        ]

        # è¶…ç¨³å®šå¤šæ ·æ€§Stemå±‚
        self.stem = self._create_ultra_stable_stem(in_channels, self.stage_channels[0])

        # Stage 1: è¶…ç¨³å®šConvNeXt
        self.stage1 = ConvNeXtStage(
            dim=self.stage_channels[0],
            depth=depths[0],
            drop_path_rate=0.0
        )
        self.downsample1 = DownsampleLayer(
            self.stage_channels[0], self.stage_channels[1]
        )

        # Stage 2: è¶…ç¨³å®šConvNeXt
        self.stage2 = ConvNeXtStage(
            dim=self.stage_channels[1],
            depth=depths[1],
            drop_path_rate=0.0
        )
        self.downsample2 = DownsampleLayer(
            self.stage_channels[1], self.stage_channels[2]
        )

        # Stage 3: è¶…ç¨³å®šVSS
        self.stage3 = VSSStage(
            dim=self.stage_channels[2],
            depth=depths[2],
            drop_path_rate=0.0,
            d_state=d_state,
        )
        self.downsample3 = DownsampleLayer(
            self.stage_channels[2], self.stage_channels[3]
        )

        # Stage 4: è¶…ç¨³å®šVSS
        self.stage4 = VSSStage(
            dim=self.stage_channels[3],
            depth=depths[3],
            drop_path_rate=0.0,
            d_state=d_state,
        )

        self._initialize_weights_ultra_conservative()
    
    def _create_ultra_stable_stem(self, in_channels, out_channels):
        """åˆ›å»ºè¶…ç¨³å®šçš„å¤šæ ·æ€§Stemå±‚"""
        
        # ä¸»åˆ†æ”¯ - æ ‡å‡†3x3å·ç§¯
        branch1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels//2, kernel_size=3, stride=2, padding=1, bias=True),
            nn.BatchNorm2d(out_channels//2),
            nn.ReLU(inplace=True)
        )
        
        # è¾…åŠ©åˆ†æ”¯ - 5x5å·ç§¯å¢åŠ æ„Ÿå—é‡
        branch2 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels//4, kernel_size=5, stride=2, padding=2, bias=True),
            nn.BatchNorm2d(out_channels//4),
            nn.ReLU(inplace=True)
        )
        
        # æ± åŒ–åˆ†æ”¯ - ä¿ç•™è¾¹ç¼˜ä¿¡æ¯
        branch3 = nn.Sequential(
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
            nn.Conv2d(in_channels, out_channels//4, kernel_size=1, bias=True),
            nn.BatchNorm2d(out_channels//4),
            nn.ReLU(inplace=True)
        )
        
        # èåˆå±‚
        fusion = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, kernel_size=1, bias=True),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
        
        class UltraStableStem(nn.Module):
            def __init__(self):
                super().__init__()
                self.branch1 = branch1
                self.branch2 = branch2
                self.branch3 = branch3
                self.fusion = fusion
                
                # å¤šæ ·æ€§å¢å¼ºå™¨ - é’ˆå¯¹ä¸åŒè¾“å…¥ç±»å‹
                self.diversity_weights = nn.Parameter(torch.ones(1, out_channels, 1, 1) * 0.01)
                
                # è¶…ä¿å®ˆåˆå§‹åŒ–
                self._init_weights()
            
            def _init_weights(self):
                for m in self.modules():
                    if isinstance(m, nn.Conv2d):
                        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                        m.weight.data *= 0.001  # ææå°çš„åˆå§‹åŒ–
                        if m.bias is not None:
                            nn.init.constant_(m.bias, 0.001)
                    elif isinstance(m, nn.BatchNorm2d):
                        nn.init.constant_(m.weight, 1)
                        nn.init.constant_(m.bias, 0)
            
            def forward(self, x):
                # æ›´å¼ºçš„è¾“å…¥é¢„å¤„ç†ï¼Œç¡®ä¿æ•°å€¼ç¨³å®š
                x = torch.clamp(x, -1.0, 1.0)
                
                # å¤šåˆ†æ”¯ç‰¹å¾æå–
                feat1 = self.branch1(x)
                feat2 = self.branch2(x)
                feat3 = self.branch3(x)
                
                # ç‰¹å¾æ‹¼æ¥
                combined = torch.cat([feat1, feat2, feat3], dim=1)
                
                # èåˆå¤„ç†
                output = self.fusion(combined)
                
                # å¢å¼ºå¤šæ ·æ€§ - é’ˆå¯¹ä¸åŒè¾“å…¥ç±»å‹
                if self.training:
                    # è®¡ç®—è¾“å…¥ç‰¹å¾
                    input_mean = torch.mean(x)
                    input_std = torch.std(x)
                    
                    # æ ¹æ®è¾“å…¥ç±»å‹è°ƒæ•´å¤šæ ·æ€§å¢å¼º
                    if input_std < 0.01:  # ä½æ–¹å·®è¾“å…¥ï¼ˆå¦‚zeros/onesï¼‰
                        # æ·»åŠ ç»“æ„åŒ–å™ªå£°
                        diversity_noise = torch.randn_like(output) * 0.05
                        # æ·»åŠ é¢‘ç‡å“åº”
                        freq_response = torch.sin(torch.arange(output.shape[-1], device=output.device).float() * 0.1)
                        freq_response = freq_response.view(1, 1, 1, -1).expand_as(output)
                        output = output + self.diversity_weights * (diversity_noise + freq_response * 0.01)
                    elif torch.abs(input_mean) < 0.01:  # æ¥è¿‘é›¶å‡å€¼
                        # å¢åŠ å¯¹æ¯”åº¦
                        contrast_enhancement = torch.tanh(output * 2.0) * 0.01
                        output = output + self.diversity_weights * contrast_enhancement
                
                return output
        
        return UltraStableStem()

    def _build_drop_path_rates(self, depths, drop_path_rate):
        """æ„å»ºdrop_pathç‡ - å…¨éƒ¨è®¾ä¸º0"""
        drop_path_rates = []
        for depth in depths:
            stage_rates = [0.0] * depth  # å¼ºåˆ¶å…¨éƒ¨ä¸º0
            drop_path_rates.append(stage_rates)
        return drop_path_rates

    def _initialize_weights_ultra_conservative(self):
        """ç»ˆæä¿å®ˆçš„æƒé‡åˆå§‹åŒ–ç­–ç•¥ - å½»åº•è§£å†³æ¢¯åº¦çˆ†ç‚¸"""
        for name, m in self.named_modules():
            if isinstance(m, nn.Conv2d):
                # æ ¹æ®å…·ä½“å±‚ä½ç½®ä½¿ç”¨ä¸åŒçš„åˆå§‹åŒ–ç­–ç•¥
                if 'stem' in name:
                    # Stemå±‚ä½¿ç”¨æ›´åŠ ä¿å®ˆçš„åˆå§‹åŒ–
                    nn.init.xavier_uniform_(m.weight, gain=0.00001)  # ææå°çš„gain
                    if m.bias is not None:
                        nn.init.constant_(m.bias, 0.00001)
                elif 'downsample' in name:
                    # ä¸‹é‡‡æ ·å±‚
                    nn.init.xavier_uniform_(m.weight, gain=0.0001) 
                    if m.bias is not None:
                        nn.init.constant_(m.bias, 0.0001)
                elif 'spatial_attention' in name or 'channel_attention' in name:
                    # æ³¨æ„åŠ›å±‚æ›´ä¿å®ˆ
                    nn.init.xavier_uniform_(m.weight, gain=0.00001)
                    if m.bias is not None:
                        nn.init.constant_(m.bias, 0.5)  # æ³¨æ„åŠ›å±‚æ­£åç½®
                else:
                    # å…¶ä»–å±‚
                    nn.init.xavier_uniform_(m.weight, gain=0.0001)
                    if m.bias is not None:
                        nn.init.constant_(m.bias, 0.0001)
                        
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
                
            elif isinstance(m, nn.LayerNorm):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            
            elif isinstance(m, nn.Linear):
                # çº¿æ€§å±‚ä¹Ÿè¦æ›´ä¿å®ˆ
                nn.init.xavier_uniform_(m.weight, gain=0.00001)
                if m.bias is not None:
                    if 'attention' in name:
                        nn.init.constant_(m.bias, 0.1)  # æ³¨æ„åŠ›å±‚æ­£åç½®
                    else:
                        nn.init.constant_(m.bias, 0.00001)

    def forward(self, x):
        """
        è¶…ç¨³å®šå‰å‘ä¼ æ’­
        """
        # è¾“å…¥é¢„å¤„ç†å’ŒèŒƒå›´é™åˆ¶
        x = torch.clamp(x, -2.0, 2.0)
        x = x * 0.25  # å¤§å¹…ç¼©æ”¾è¾“å…¥
        
        # Stemå¤„ç†
        x = self.stem(x)  # (B, C, H/2, W/2)

        # Stage 1
        x = self.stage1(x)
        x_enc1 = self.downsample1(x)  # (B, 2C, H/4, W/4)

        # Stage 2
        x = self.stage2(x_enc1)
        x_enc2 = self.downsample2(x)  # (B, 4C, H/8, W/8)

        # Stage 3
        x = self.stage3(x_enc2)
        x_enc3 = self.downsample3(x)  # (B, 8C, H/16, W/16)

        # Stage 4
        x_enc4 = self.stage4(x_enc3)  # (B, 8C, H/16, W/16)

        return [x_enc1, x_enc2, x_enc3, x_enc4]

    def get_feature_channels(self):
        """è·å–ç‰¹å¾é€šé“ä¿¡æ¯"""
        return {
            "enc1": self.stage_channels[1],
            "enc2": self.stage_channels[2],
            "enc3": self.stage_channels[3],
            "enc4": self.stage_channels[4],
        }


def create_hybrid_encoder(
    in_channels=3, 
    base_channels=32, 
    depths=[2, 2, 2, 2], 
    drop_path_rate=0.0,  # å¼ºåˆ¶ç¦ç”¨
    d_state=16,
    encoder_config='base',
    **kwargs
):
    """åˆ›å»ºè¶…ç¨³å®šæ··åˆç¼–ç å™¨"""
    return HybridEncoder(
        in_channels=in_channels,
        base_channels=base_channels,
        depths=depths,
        drop_path_rate=0.0,  # å¼ºåˆ¶ç¦ç”¨
        d_state=d_state,
        encoder_config=encoder_config,
        **kwargs
    )


def hybrid_encoder_base(in_channels=3, **kwargs):
    """åŸºç¡€ç¼–ç å™¨é…ç½® - è¶…ç¨³å®šç‰ˆ"""
    filtered_kwargs = {k: v for k, v in kwargs.items() 
                      if k not in ['base_channels', 'depths', 'drop_path_rate', 'd_state', 'encoder_config']}
    
    return create_hybrid_encoder(
        in_channels=in_channels,
        base_channels=32,
        depths=[2, 2, 2, 2],
        drop_path_rate=0.0,  # å®Œå…¨ç¦ç”¨
        d_state=12,  # é™ä½d_state
        encoder_config='base',
        **filtered_kwargs
    )


# =============================================================================
# å‘åå…¼å®¹çš„åˆ«å - ä¿æŒæ‚¨çš„ç±»åä¸å˜
# =============================================================================

HybridEncoderV2 = HybridEncoder
create_encoder = create_hybrid_encoder
encoder_base = hybrid_encoder_base


# =============================================================================
# æµ‹è¯•å‡½æ•°
# =============================================================================

def test_ultra_stable_encoder():
    """æµ‹è¯•è¶…ç¨³å®šç¼–ç å™¨"""
    print("ğŸ¯ æµ‹è¯•è¶…ç¨³å®šæ··åˆç¼–ç å™¨...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åˆ›å»ºç¼–ç å™¨
    encoder = hybrid_encoder_base(in_channels=3).to(device)
    encoder.train()  # è®­ç»ƒæ¨¡å¼æµ‹è¯•
    
    # æµ‹è¯•ä¸åŒè¾“å…¥ - é’ˆå¯¹ç‰¹å¾å¤šæ ·æ€§é—®é¢˜
    test_cases = {
        'random': torch.randn(1, 3, 256, 256).to(device),
        'zeros': torch.zeros(1, 3, 256, 256).to(device),
        'ones': torch.ones(1, 3, 256, 256).to(device),
        'checkerboard': torch.zeros(1, 3, 256, 256).to(device),
        'small_values': torch.randn(1, 3, 256, 256).to(device) * 0.01,
    }
    
    # åˆ›å»ºæ£‹ç›˜å›¾æ¡ˆ
    for i in range(0, 256, 32):
        for j in range(0, 256, 32):
            if (i // 32 + j // 32) % 2 == 0:
                test_cases['checkerboard'][:, :, i:i+32, j:j+32] = 1
    
    print("æµ‹è¯•ä¸åŒè¾“å…¥ç±»å‹çš„ç‰¹å¾å¤šæ ·æ€§:")
    
    success_count = 0
    for case_name, test_input in test_cases.items():
        try:
            # å‰å‘ä¼ æ’­
            features = encoder(test_input)
            
            # æ£€æŸ¥stemç‰¹å¾å¤šæ ·æ€§
            stem_features = encoder.stem(test_input)
            stem_np = stem_features.detach().cpu().numpy()
            
            unique_ratio = len(np.unique(stem_np)) / stem_np.size
            mean_val = np.mean(np.abs(stem_np))
            std_val = np.std(stem_np)
            
            print(f"  {case_name:12s}: å¤šæ ·æ€§={unique_ratio:.4f}, å‡å€¼={mean_val:.6f}, æ ‡å‡†å·®={std_val:.6f}")
            
            # æ£€æŸ¥å¤šæ ·æ€§é˜ˆå€¼
            if unique_ratio > 0.001 and mean_val > 1e-6 and std_val > 1e-6:
                success_count += 1
                print(f"  {case_name:12s}: âœ… ç‰¹å¾å¤šæ ·æ€§æ­£å¸¸")
            else:
                print(f"  {case_name:12s}: âš ï¸ ç‰¹å¾å¤šæ ·æ€§ä¸è¶³")
            
        except Exception as e:
            print(f"  {case_name:12s}: âŒ å¤±è´¥: {e}")
    
    print(f"\nè¶…ç¨³å®šç¼–ç å™¨æµ‹è¯•: {success_count}/{len(test_cases)} æˆåŠŸ")
    return success_count == len(test_cases)


def test_encoder_performance():
    """æ€§èƒ½æµ‹è¯•"""
    return test_ultra_stable_encoder()


def test_channel_attention():
    """é€šé“æ³¨æ„åŠ›æµ‹è¯•"""
    print("ğŸ¯ æµ‹è¯•è¶…ç¨³å®šé€šé“æ³¨æ„åŠ›...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    x = torch.randn(2, 64, 32, 32).to(device)
    ca = ChannelAttention(64, reduction=16).to(device)
    
    with torch.no_grad():
        out_ca = ca(x)
    
    output_mean = torch.mean(out_ca).item()
    output_std = torch.std(out_ca).item()
    
    print(f"   è¾“å‡ºç»Ÿè®¡: å‡å€¼={output_mean:.6f}, æ ‡å‡†å·®={output_std:.6f}")
    
    if output_mean > 1e-6 and output_std > 1e-6:
        print("âœ… è¶…ç¨³å®šé€šé“æ³¨æ„åŠ›æµ‹è¯•é€šè¿‡")
        return True
    else:
        print("âŒ é€šé“æ³¨æ„åŠ›è¾“å‡ºå¼‚å¸¸")
        return False


if __name__ == "__main__":
    import numpy as np
    test_channel_attention()
    test_ultra_stable_encoder()
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import numpy as np
from typing import Optional, Tuple, Union, Dict, List
from ..HMA_UNet import HMAUNet


# ====================================é›†æˆæ‰©æ•£è°ƒåº¦å™¨=========================================
class IntegratedNoiseScheduler:
    """é›†æˆå™ªå£°è°ƒåº¦å™¨ - é’ˆå¯¹ç«¯åˆ°ç«¯è®­ç»ƒä¼˜åŒ–"""
    
    def __init__(self, timesteps: int = 1000, beta_start: float = 0.0001, beta_end: float = 0.02):
        self.timesteps = timesteps
        
        # ä½™å¼¦è°ƒåº¦ - æ›´é€‚åˆç«¯åˆ°ç«¯è®­ç»ƒ
        s = 0.008
        steps = timesteps + 1
        x = torch.linspace(0, timesteps, steps)
        alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2
        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
        
        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
        self.betas = torch.clip(betas, 0.0001, 0.9999)
        
        self.alphas = 1.0 - self.betas
        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)
        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)
        
        # è®¡ç®—å»å™ªæ‰€éœ€çš„ç³»æ•°
        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)
        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)
        self.sqrt_recip_alphas = torch.sqrt(1.0 / self.alphas)
        
        # åéªŒåˆ†å¸ƒæ–¹å·®
        self.posterior_variance = (
            self.betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)
        )
    
    def to(self, device):
        """ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡"""
        self.betas = self.betas.to(device)
        self.alphas = self.alphas.to(device)
        self.alphas_cumprod = self.alphas_cumprod.to(device)
        self.alphas_cumprod_prev = self.alphas_cumprod_prev.to(device)
        self.sqrt_alphas_cumprod = self.sqrt_alphas_cumprod.to(device)
        self.sqrt_one_minus_alphas_cumprod = self.sqrt_one_minus_alphas_cumprod.to(device)
        self.sqrt_recip_alphas = self.sqrt_recip_alphas.to(device)
        self.posterior_variance = self.posterior_variance.to(device)
        return self
    
    def add_noise(self, x_start, noise, timesteps):
        """æ·»åŠ å™ªå£°"""
        device = x_start.device
        timesteps = timesteps.to(device)
        
        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[timesteps].reshape(-1, 1, 1, 1)
        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[timesteps].reshape(-1, 1, 1, 1)
        
        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise
    
    def denoise_step(self, model_output, timestep, sample):
        """å•æ­¥å»å™ª"""
        device = sample.device
        timestep = timestep.to(device)
        
        alpha_prod_t = self.alphas_cumprod[timestep].reshape(-1, 1, 1, 1)
        alpha_prod_t_prev = self.alphas_cumprod_prev[timestep].reshape(-1, 1, 1, 1)
        beta_prod_t = 1 - alpha_prod_t
        beta_prod_t_prev = 1 - alpha_prod_t_prev
        
        # é¢„æµ‹åŸå§‹æ ·æœ¬
        pred_original_sample = (sample - torch.sqrt(beta_prod_t) * model_output) / torch.sqrt(alpha_prod_t)
        pred_original_sample = torch.clamp(pred_original_sample, 0, 1)
        
        # è®¡ç®—å‰ä¸€æ­¥æ ·æœ¬
        pred_sample_direction = torch.sqrt(beta_prod_t_prev) * model_output
        prev_sample = torch.sqrt(alpha_prod_t_prev) * pred_original_sample + pred_sample_direction
        prev_sample = torch.clamp(prev_sample, 0, 1)
        
        return prev_sample


# =============================================================================
# é›†æˆç½‘ç»œç»„ä»¶
# =============================================================================

class SinusoidalTimeEmbedding(nn.Module):
    """æ­£å¼¦æ—¶é—´åµŒå…¥"""
    
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim
        
    def forward(self, timestep: torch.Tensor) -> torch.Tensor:
        device = timestep.device
        half_dim = self.dim // 2
        
        emb = math.log(10000) / (half_dim - 1)
        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)
        emb = timestep[:, None] * emb[None, :]
        emb = torch.cat([emb.sin(), emb.cos()], dim=-1)
        
        if emb.shape[-1] < self.dim:
            padding = self.dim - emb.shape[-1]
            emb = F.pad(emb, (0, padding))
        elif emb.shape[-1] > self.dim:
            emb = emb[:, :self.dim]
        
        return emb


class TimeConditionedConv(nn.Module):
    """æ—¶é—´æ¡ä»¶å·ç§¯å±‚"""
    
    def __init__(self, in_channels: int, out_channels: int, time_emb_dim: int, **kwargs):
        super().__init__()
        
        kernel_size = kwargs.get('kernel_size', 3)
        padding = kwargs.get('padding', 1)
        
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding)
        
        self.time_proj = nn.Sequential(
            nn.Linear(time_emb_dim, time_emb_dim),
            nn.SiLU(),
            nn.Linear(time_emb_dim, out_channels)
        )
        
        self.norm = nn.GroupNorm(min(8, max(1, out_channels//4)), out_channels)
        
    def forward(self, x, time_emb):
        h = self.conv(x)
        h = self.norm(h)
        
        time_cond = self.time_proj(time_emb)
        time_cond = time_cond[:, :, None, None]
        h = h + time_cond
        
        return F.silu(h)


class CrossScaleAttention(nn.Module):
    """è·¨å°ºåº¦æ³¨æ„åŠ› - è¿æ¥HMA-UNetç‰¹å¾ä¸æ‰©æ•£ç‰¹å¾"""
    
    def __init__(self, diffusion_dim: int, hma_dim: int):
        super().__init__()
        
        self.diffusion_dim = diffusion_dim
        self.hma_dim = hma_dim
        
        # ç‰¹å¾é€‚é…
        if hma_dim != diffusion_dim:
            self.feature_adapter = nn.Conv2d(hma_dim, diffusion_dim, kernel_size=1)
        else:
            self.feature_adapter = nn.Identity()
        
        # è·¨æ¨¡æ€æ³¨æ„åŠ›
        self.cross_attn = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(diffusion_dim * 2, diffusion_dim // 4, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(diffusion_dim // 4, diffusion_dim, kernel_size=1),
            nn.Sigmoid()
        )
        
    def forward(self, diffusion_features, hma_features):
        """
        Args:
            diffusion_features: (B, C1, H, W) - æ‰©æ•£æ¨¡å‹ç‰¹å¾
            hma_features: (B, C2, H', W') - HMA-UNetç‰¹å¾
        """
        B, C, H, W = diffusion_features.shape
        
        # è°ƒæ•´HMAç‰¹å¾çš„ç©ºé—´ç»´åº¦
        if hma_features.shape[2:] != (H, W):
            hma_features = F.interpolate(hma_features, size=(H, W), mode='bilinear', align_corners=False)
        
        # é€šé“é€‚é…
        hma_features = self.feature_adapter(hma_features)
        
        # èåˆç‰¹å¾
        combined = torch.cat([diffusion_features, hma_features], dim=1)
        
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attn_weights = self.cross_attn(combined)
        
        # åº”ç”¨æ³¨æ„åŠ›
        enhanced_diffusion = diffusion_features * attn_weights
        enhanced_hma = hma_features * (1 - attn_weights)
        
        return enhanced_diffusion + enhanced_hma


# =============================================================================
# é›†æˆæ‰©æ•£ç²¾ç‚¼ç½‘ç»œ
# =============================================================================

class IntegratedDiffusionUNet(nn.Module):
    """é›†æˆæ‰©æ•£U-Net - ä¸HMA-UNetååŒå·¥ä½œ"""
    
    def __init__(
        self,
        in_channels: int = 2,  # [noisy_mask, initial_mask]
        out_channels: int = 1,
        base_channels: int = 32,
        time_emb_dim: int = 128,
        hma_channels: List[int] = [64, 128, 256, 256]
    ):
        super().__init__()
        
        self.base_channels = base_channels
        self.time_emb_dim = time_emb_dim
        
        # æ—¶é—´åµŒå…¥
        self.time_embedding = SinusoidalTimeEmbedding(time_emb_dim)
        
        # ç¼–ç å™¨
        encoder_channels = [base_channels, base_channels*2, base_channels*4, base_channels*8]
        self.encoder = nn.ModuleList([
            TimeConditionedConv(in_channels, encoder_channels[0], time_emb_dim),
            TimeConditionedConv(encoder_channels[0], encoder_channels[1], time_emb_dim),
            TimeConditionedConv(encoder_channels[1], encoder_channels[2], time_emb_dim),
            TimeConditionedConv(encoder_channels[2], encoder_channels[3], time_emb_dim),
        ])
        
        # ä¸‹é‡‡æ ·
        self.downsample = nn.ModuleList([
            nn.AvgPool2d(2) for _ in range(4)
        ])
        
        # è·¨å°ºåº¦æ³¨æ„åŠ›å±‚ - è¿æ¥HMA-UNet
        self.cross_scale_attention = nn.ModuleList([
            CrossScaleAttention(encoder_channels[0], hma_channels[0]),
            CrossScaleAttention(encoder_channels[1], hma_channels[1]),
            CrossScaleAttention(encoder_channels[2], hma_channels[2]),
            CrossScaleAttention(encoder_channels[3], hma_channels[3]),
        ])
        
        # ç“¶é¢ˆå±‚
        self.bottleneck = nn.Sequential(
            TimeConditionedConv(encoder_channels[3], encoder_channels[3], time_emb_dim),
            TimeConditionedConv(encoder_channels[3], encoder_channels[3], time_emb_dim),
        )
        
        # è§£ç å™¨
        decoder_channels = [encoder_channels[2], encoder_channels[1], encoder_channels[0], encoder_channels[0]]
        self.decoder = nn.ModuleList([
            TimeConditionedConv(encoder_channels[3] + encoder_channels[3], decoder_channels[0], time_emb_dim),
            TimeConditionedConv(decoder_channels[0] + encoder_channels[2], decoder_channels[1], time_emb_dim),
            TimeConditionedConv(decoder_channels[1] + encoder_channels[1], decoder_channels[2], time_emb_dim),
            TimeConditionedConv(decoder_channels[2] + encoder_channels[0], decoder_channels[3], time_emb_dim),
        ])
        
        # ä¸Šé‡‡æ ·
        self.upsample = nn.ModuleList([
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False) for _ in range(4)
        ])
        
        # è¾“å‡ºå±‚
        self.output_conv = nn.Conv2d(decoder_channels[3], out_channels, kernel_size=1)
        
        print(f"ğŸ—ï¸ é›†æˆæ‰©æ•£U-Neté…ç½®:")
        print(f"   ç¼–ç å™¨é€šé“: {encoder_channels}")
        print(f"   è§£ç å™¨é€šé“: {decoder_channels}")
        print(f"   HMAè¿æ¥é€šé“: {hma_channels}")
        
    def forward(self, x, time_emb, hma_features):
        """
        Args:
            x: (B, 2, H, W) - [noisy_mask, initial_mask]
            time_emb: (B, time_emb_dim)
            hma_features: Dict[str, torch.Tensor] - HMA-UNetç‰¹å¾
        """
        # ç¼–ç å™¨è·¯å¾„
        encoder_features = []
        h = x
        
        hma_keys = ['encoder_stage1', 'encoder_stage2', 'encoder_stage3', 'encoder_stage4']
        
        for i, (enc_layer, down, cross_attn) in enumerate(zip(
            self.encoder, self.downsample, self.cross_scale_attention
        )):
            h = enc_layer(h, time_emb)
            
            # è·¨å°ºåº¦æ³¨æ„åŠ›èåˆHMAç‰¹å¾
            if hma_keys[i] in hma_features:
                h = cross_attn(h, hma_features[hma_keys[i]])
            
            encoder_features.append(h)
            h = down(h)
        
        # ç“¶é¢ˆå±‚
        for bottleneck_layer in self.bottleneck:
            h = bottleneck_layer(h, time_emb)
        
        # è§£ç å™¨è·¯å¾„
        for i, (dec_layer, up) in enumerate(zip(self.decoder, self.upsample)):
            h = up(h)
            
            # è·³è·ƒè¿æ¥
            skip = encoder_features[-(i+1)]
            h = torch.cat([h, skip], dim=1)
            
            # è§£ç å™¨å·ç§¯
            h = dec_layer(h, time_emb)
        
        # è¾“å‡º
        output = self.output_conv(h)
        return output


# =============================================================================
# ä¸»è¦é›†æˆæ¨¡å‹
# =============================================================================

class IntegratedHMADRM(nn.Module):
    """é›†æˆHMA-UNetä¸DRMçš„ç«¯åˆ°ç«¯æ¨¡å‹"""
    
    def __init__(
        self,
        in_channels: int = 3,
        num_classes: int = 1,
        base_channels: int = 32,
        timesteps: int = 1000,
        use_diffusion_training: bool = True,
        diffusion_probability: float = 0.5,
        device: str = "cuda"
    ):
        super().__init__()
        
        self.timesteps = timesteps
        self.device = torch.device(device)
        self.use_diffusion_training = use_diffusion_training
        self.diffusion_probability = diffusion_probability
        self.time_emb_dim = base_channels * 4
        
        print(f"ğŸ”§ åˆå§‹åŒ–é›†æˆHMA-DRMæ¨¡å‹...")
        print(f"   ç›®æ ‡è®¾å¤‡: {device}")
        print(f"   æ‰©æ•£è®­ç»ƒ: {'å¯ç”¨' if use_diffusion_training else 'ç¦ç”¨'}")
        print(f"   æ‰©æ•£æ¦‚ç‡: {diffusion_probability}")
        print(f"   æ—¶é—´æ­¥æ•°: {timesteps}")
        
        # 1. HMA-UNetæ ¸å¿ƒ (å¯è®­ç»ƒ) - ç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        print("ğŸ“¦ åˆ›å»ºHMA-UNetæ ¸å¿ƒ...")
        from ..HMA_UNet import create_hma_unet
        self.hma_unet = create_hma_unet(
            config="base",
            in_channels=in_channels,
            num_classes=num_classes,
            base_channels=base_channels
        )
        
        # ç«‹å³ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡
        self.hma_unet = self.hma_unet.to(self.device)
        print(f"âœ… HMA-UNetå·²ç§»åŠ¨åˆ°è®¾å¤‡: {next(self.hma_unet.parameters()).device}")
        
        # 2. æ‰©æ•£è°ƒåº¦å™¨
        print("â° åˆ›å»ºæ‰©æ•£è°ƒåº¦å™¨...")
        self.noise_scheduler = IntegratedNoiseScheduler(timesteps=timesteps).to(self.device)
        
        # 3. è·å–HMA-UNeté€šé“é…ç½® - åœ¨æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡å
        print("ğŸ” æ£€æµ‹HMA-UNeté€šé“é…ç½®...")
        self.hma_channels = self._get_hma_channels()
        print(f"   HMAé€šé“é…ç½®: {self.hma_channels}")
        
        # 4. é›†æˆæ‰©æ•£ç²¾ç‚¼ç½‘ç»œ
        print("ğŸŒŠ åˆ›å»ºé›†æˆæ‰©æ•£ç²¾ç‚¼ç½‘ç»œ...")
        self.diffusion_unet = IntegratedDiffusionUNet(
            in_channels=2,  # [noisy_mask, initial_mask]
            out_channels=1,
            base_channels=base_channels,
            time_emb_dim=self.time_emb_dim,
            hma_channels=self.hma_channels
        )
        
        # ç«‹å³ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡
        self.diffusion_unet = self.diffusion_unet.to(self.device)
        print(f"âœ… æ‰©æ•£ç½‘ç»œå·²ç§»åŠ¨åˆ°è®¾å¤‡: {next(self.diffusion_unet.parameters()).device}")
        
        print("âœ… é›†æˆæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        
    def _get_hma_channels(self) -> List[int]:
        """è·å–HMA-UNetçš„é€šé“é…ç½® - ä¿®å¤è®¾å¤‡åŒæ­¥"""
        # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        device = next(self.hma_unet.parameters()).device
        test_input = torch.randn(1, 3, 256, 256).to(device)
        
        print(f"ğŸ” é€šé“æ£€æµ‹ - è¾“å…¥è®¾å¤‡: {test_input.device}, æ¨¡å‹è®¾å¤‡: {device}")
        
        with torch.no_grad():
            self.hma_unet.eval()
            try:
                encoder_features = self.hma_unet.encoder(test_input)
                x_enc1, x_enc2, x_enc3, x_enc4 = encoder_features
                
                channels = [
                    x_enc1.shape[1],  # encoder_stage1
                    x_enc2.shape[1],  # encoder_stage2
                    x_enc3.shape[1],  # encoder_stage3
                    x_enc4.shape[1],  # encoder_stage4
                ]
                
                print(f"âœ… é€šé“æ£€æµ‹æˆåŠŸ: {channels}")
                print(f"   ç‰¹å¾å½¢çŠ¶: enc1={x_enc1.shape}, enc2={x_enc2.shape}, enc3={x_enc3.shape}, enc4={x_enc4.shape}")
                
                return channels
                
            except Exception as e:
                print(f"âŒ é€šé“æ£€æµ‹å¤±è´¥: {e}")
                # ä½¿ç”¨é»˜è®¤é€šé“é…ç½®
                default_channels = [64, 128, 256, 256]  # baseé…ç½®çš„é»˜è®¤é€šé“
                print(f"âš ï¸ ä½¿ç”¨é»˜è®¤é€šé“é…ç½®: {default_channels}")
                return default_channels
    
    def extract_hma_features(self, images: torch.Tensor) -> Dict[str, torch.Tensor]:
        """æå–HMA-UNetç‰¹å¾ - ç¡®ä¿è®¾å¤‡ä¸€è‡´æ€§"""
        # ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        images = images.to(next(self.hma_unet.parameters()).device)
        
        # è·å–ç¼–ç å™¨ç‰¹å¾
        encoder_features = self.hma_unet.encoder(images)
        x_enc1, x_enc2, x_enc3, x_enc4 = encoder_features
        
        # è·å–ç“¶é¢ˆå±‚ç‰¹å¾
        bottleneck_features = self.hma_unet.bottleneck(x_enc4)
        
        return {
            'encoder_stage1': x_enc1,
            'encoder_stage2': x_enc2,
            'encoder_stage3': x_enc3,
            'encoder_stage4': x_enc4,
            'bottleneck': bottleneck_features
        }
    
    def get_initial_prediction(self, images: torch.Tensor) -> torch.Tensor:
        """è·å–HMA-UNetçš„åˆå§‹é¢„æµ‹ - ç¡®ä¿è®¾å¤‡ä¸€è‡´æ€§"""
        images = images.to(next(self.hma_unet.parameters()).device)
        return torch.sigmoid(self.hma_unet(images))
    
    def forward(self, images: torch.Tensor, target_masks: torch.Tensor = None, mode: str = "train"):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            images: è¾“å…¥å›¾åƒ (B, 3, H, W)
            target_masks: ç›®æ ‡æ©ç  (B, 1, H, W) - ä»…è®­ç»ƒæ—¶éœ€è¦
            mode: "train" æˆ– "inference"
        """
        # ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        images = images.to(self.device)
        if target_masks is not None:
            target_masks = target_masks.to(self.device)
            
        if mode == "train":
            return self._forward_train(images, target_masks)
        else:
            return self._forward_inference(images)
    
    def _forward_train(self, images: torch.Tensor, target_masks: torch.Tensor):
        """è®­ç»ƒæ—¶çš„å‰å‘ä¼ æ’­"""
        device = images.device
        batch_size = images.shape[0]
        
        # 1. æå–HMAç‰¹å¾
        hma_features = self.extract_hma_features(images)
        
        # 2. è·å–åˆå§‹é¢„æµ‹
        initial_prediction = self.get_initial_prediction(images)
        
        # 3. å†³å®šæ˜¯å¦ä½¿ç”¨æ‰©æ•£è®­ç»ƒ
        use_diffusion = self.use_diffusion_training and (torch.rand(1).item() < self.diffusion_probability)
        
        if use_diffusion:
            # æ‰©æ•£è®­ç»ƒè·¯å¾„
            # éšæœºé‡‡æ ·æ—¶é—´æ­¥
            timesteps = torch.randint(0, self.timesteps // 2, (batch_size,), device=device, dtype=torch.long)
            
            # ç”Ÿæˆå™ªå£°
            noise = torch.randn_like(target_masks)
            
            # å‰å‘æ‰©æ•£
            noisy_masks = self.noise_scheduler.add_noise(target_masks, noise, timesteps)
            
            # æ—¶é—´åµŒå…¥
            time_emb = self.diffusion_unet.time_embedding(timesteps.float())
            
            # æ‰©æ•£ç½‘ç»œè¾“å…¥
            diffusion_input = torch.cat([noisy_masks, initial_prediction], dim=1)
            
            # é¢„æµ‹å™ªå£°
            predicted_noise = self.diffusion_unet(diffusion_input, time_emb, hma_features)
            
            return {
                'initial_prediction': initial_prediction,
                'predicted_noise': predicted_noise,
                'target_noise': noise,
                'mode': 'diffusion'
            }
        else:
            # ç›´æ¥åˆ†å‰²è®­ç»ƒè·¯å¾„
            return {
                'initial_prediction': initial_prediction,
                'mode': 'direct'
            }
    
    def _forward_inference(self, images: torch.Tensor, num_inference_steps: int = 50):
        """æ¨ç†æ—¶çš„å‰å‘ä¼ æ’­"""
        device = images.device
        batch_size = images.shape[0]
        
        # 1. æå–HMAç‰¹å¾
        hma_features = self.extract_hma_features(images)
        
        # 2. è·å–åˆå§‹é¢„æµ‹
        initial_prediction = self.get_initial_prediction(images)
        
        if not self.use_diffusion_training:
            # ä»…è¿”å›åˆå§‹é¢„æµ‹
            return initial_prediction
        
        # 3. æ‰©æ•£ç²¾ç‚¼è¿‡ç¨‹
        current_mask = initial_prediction + 0.05 * torch.randn_like(initial_prediction)
        current_mask = torch.clamp(current_mask, 0, 1)
        
        # æ—¶é—´æ­¥åºåˆ—
        max_timestep = self.timesteps // 4
        timesteps = torch.linspace(max_timestep-1, 0, num_inference_steps, dtype=torch.long, device=device)
        
        # é€æ­¥å»å™ª
        for i, t in enumerate(timesteps):
            t_batch = t.expand(batch_size)
            time_emb = self.diffusion_unet.time_embedding(t_batch.float())
            
            # æ‰©æ•£ç½‘ç»œè¾“å…¥
            diffusion_input = torch.cat([current_mask, initial_prediction], dim=1)
            
            # é¢„æµ‹å™ªå£°
            with torch.no_grad():
                predicted_noise = self.diffusion_unet(diffusion_input, time_emb, hma_features)
            
            # å»å™ªæ­¥éª¤
            current_mask = self.noise_scheduler.denoise_step(predicted_noise, t_batch, current_mask)
            current_mask = torch.clamp(current_mask, 0, 1)
            
            # æ¸è¿›å¼æ··åˆ
            alpha = 0.8 + 0.2 * (i / len(timesteps))
            current_mask = alpha * current_mask + (1 - alpha) * initial_prediction
        
        return current_mask
    
    def compute_loss(self, images: torch.Tensor, target_masks: torch.Tensor):
        """è®¡ç®—è®­ç»ƒæŸå¤±"""
        outputs = self.forward(images, target_masks, mode="train")
        
        if outputs['mode'] == 'diffusion':
            # æ‰©æ•£æŸå¤±
            noise_loss = F.mse_loss(outputs['predicted_noise'], outputs['target_noise'])
            initial_loss = F.binary_cross_entropy(outputs['initial_prediction'], target_masks)
            
            total_loss = noise_loss + 0.1 * initial_loss
            
            return {
                'total_loss': total_loss,
                'noise_loss': noise_loss,
                'initial_loss': initial_loss,
                'mode': 'diffusion'
            }
        else:
            # ç›´æ¥åˆ†å‰²æŸå¤±
            initial_loss = F.binary_cross_entropy(outputs['initial_prediction'], target_masks)
            
            return {
                'total_loss': initial_loss,
                'initial_loss': initial_loss,
                'mode': 'direct'
            }
    
    def to(self, device):
        """é‡å†™toæ–¹æ³•ç¡®ä¿æ‰€æœ‰ç»„ä»¶éƒ½ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡"""
        super().to(device)
        self.device = torch.device(device)
        
        # ç§»åŠ¨æ‰€æœ‰å­æ¨¡å—
        self.hma_unet = self.hma_unet.to(device)
        self.diffusion_unet = self.diffusion_unet.to(device)
        self.noise_scheduler = self.noise_scheduler.to(device)
        
        print(f"ğŸ“± æ¨¡å‹å·²å®Œå…¨ç§»åŠ¨åˆ°è®¾å¤‡: {device}")
        return self


# =============================================================================
# å·¥å‚å‡½æ•°
# =============================================================================

def create_integrated_hma_drm(
    in_channels: int = 3,
    num_classes: int = 1,
    base_channels: int = 32,
    timesteps: int = 1000,
    use_diffusion_training: bool = True,
    diffusion_probability: float = 0.5,
    device: str = "cuda",
    **kwargs
) -> IntegratedHMADRM:
    """åˆ›å»ºé›†æˆHMA-DRMæ¨¡å‹çš„å·¥å‚å‡½æ•°"""
    
    print(f"ğŸ­ åˆ›å»ºé›†æˆHMA-DRMæ¨¡å‹...")
    print(f"   ç›®æ ‡è®¾å¤‡: {device}")
    
    model = IntegratedHMADRM(
        in_channels=in_channels,
        num_classes=num_classes,
        base_channels=base_channels,
        timesteps=timesteps,
        use_diffusion_training=use_diffusion_training,
        diffusion_probability=diffusion_probability,
        device=device
    )
    
    # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
    model = model.to(device)
    
    print(f"âœ… é›†æˆHMA-DRMæ¨¡å‹åˆ›å»ºå®Œæˆï¼Œè®¾å¤‡: {next(model.parameters()).device}")
    
    return model
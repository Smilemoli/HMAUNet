import torch
import torch.nn as nn
import traceback
import time
import sys
import os

# Á°Æ‰øù‰ªéÊ≠£Á°ÆÁöÑÁõÆÂΩïËøêË°å
if os.getcwd().endswith("models"):
    os.chdir("..")  # ÂàáÊç¢Âà∞È°πÁõÆÊ†πÁõÆÂΩï

# Ê∑ªÂä†È°πÁõÆÊ†πÁõÆÂΩïÂà∞PythonË∑ØÂæÑ
project_root = os.getcwd()
if project_root not in sys.path:
    sys.path.insert(0, project_root)

print("Current working directory:", os.getcwd())
print("Python path:", sys.path[:3])

# Âº∫Âà∂Ê£ÄÊü•CUDAÂèØÁî®ÊÄß
def check_cuda_availability():
    """Ê£ÄÊü•CUDAÂèØÁî®ÊÄßÔºåÂ¶ÇÊûú‰∏çÂèØÁî®ÂàôÈÄÄÂá∫"""
    if not torch.cuda.is_available():
        print("‚ùå CUDA is not available!")
        print("‚ùå This test suite requires CUDA for Mamba support.")
        print("‚ùå Please ensure:")
        print("   1. NVIDIA GPU is available")
        print("   2. CUDA toolkit is installed")
        print("   3. PyTorch with CUDA support is installed")
        sys.exit(1)
    
    print(f"‚úÖ CUDA is available: {torch.cuda.get_device_name(0)}")
    print(f"‚úÖ CUDA version: {torch.version.cuda}")
    return True

def test_imports():
    """ÊµãËØïÂØºÂÖ•ÊòØÂê¶Ê≠£Â∏∏Â∑•‰Ωú"""
    print("=" * 80)
    print("Import Test")
    print("=" * 80)

    # È¶ñÂÖàÊ£ÄÊü•CUDA
    check_cuda_availability()

    try:
        # ÊµãËØïÂêÑ‰∏™ÁªÑ‰ª∂ÁöÑÂØºÂÖ•
        print("üß© Testing component imports...")

        # È¶ñÂÖàÊµãËØïÂü∫Á°ÄÊ®°Âùó
        try:
            from models.backbones.ResNet_blocks import (
                ResidualBlock,
                ConvBlock,
                TransposeConvBlock,
            )
            print("   ‚úÖ ResNet blocks imported successfully")
        except Exception as e:
            print(f"   ‚ùå ResNet blocks import failed: {e}")
            return False

        try:
            from models.backbones.convnext_blocks import ConvNeXtV2Block
            print("   ‚úÖ ConvNeXt blocks imported successfully")
        except Exception as e:
            print(f"   ‚ùå ConvNeXt blocks import failed: {e}")
            return False

        try:
            from models.backbones.vss_blocks import VSSBlock
            print("   ‚úÖ VSS blocks imported successfully")
        except Exception as e:
            print(f"   ‚ùå VSS blocks import failed: {e}")
            return False

        try:
            from models.components.csfg_module import CSFGModule
            print("   ‚úÖ CSFGModule imported successfully")
        except Exception as e:
            print(f"   ‚ùå CSFGModule import failed: {e}")
            return False

        try:
            from models.components.encoder import HybridEncoder
            print("   ‚úÖ HybridEncoder imported successfully")
        except Exception as e:
            print(f"   ‚ùå HybridEncoder import failed: {e}")
            return False

        try:
            from models.components.hma_module import HMABottleneck
            print("   ‚úÖ HMABottleneck imported successfully")
        except Exception as e:
            print(f"   ‚ùå HMABottleneck import failed: {e}")
            return False

        try:
            from models.components.decoder import ResNetDecoder, OutputHead
            print("   ‚úÖ ResNetDecoder imported successfully")
        except Exception as e:
            print(f"   ‚ùå ResNetDecoder import failed: {e}")
            return False

        print("   ‚úÖ All component imports test PASSED")
        return True

    except Exception as e:
        print(f"   ‚ùå Components test FAILED: {e}")
        traceback.print_exc()
        return False


def create_hma_unet_local(config="tiny", in_channels=3, num_classes=1, **kwargs):
    """Êú¨Âú∞ÂàõÂª∫HMA-UNetÊ®°ÂûãÁöÑÂáΩÊï∞ - CUDA only"""
    from models.components.encoder import HybridEncoder
    from models.components.hma_module import HMABottleneck
    from models.components.decoder import ResNetDecoder, OutputHead

    # Ê†πÊçÆÈÖçÁΩÆËÆæÁΩÆÂèÇÊï∞
    if config == "tiny":
        base_channels = 32
        encoder_depths = [2, 2, 2, 2]
        encoder_drop_path_rate = 0.1
        bottleneck_num_levels = 3
        bottleneck_drop_path_rate = 0.15
        csfg_reduction_ratio = 8
        decoder_num_res_blocks = 2
        d_state = 16
        dropout = 0.1
    elif config == "small":
        base_channels = 48
        encoder_depths = [2, 2, 4, 2]
        encoder_drop_path_rate = 0.15
        bottleneck_num_levels = 3
        bottleneck_drop_path_rate = 0.2
        csfg_reduction_ratio = 6
        decoder_num_res_blocks = 3
        d_state = 16
        dropout = 0.1
    elif config == "base":
        base_channels = 64
        encoder_depths = [3, 3, 6, 3]
        encoder_drop_path_rate = 0.2
        bottleneck_num_levels = 4
        bottleneck_drop_path_rate = 0.25
        csfg_reduction_ratio = 4
        decoder_num_res_blocks = 3
        d_state = 24
        dropout = 0.1
    else:
        raise ValueError(f"Unknown config: {config}")

    # Â∫îÁî®È¢ùÂ§ñÂèÇÊï∞
    for key, value in kwargs.items():
        locals()[key] = value

    class HMAUNetLocal(nn.Module):
        def __init__(self):
            super().__init__()

            self.in_channels = in_channels
            self.num_classes = num_classes
            self.base_channels = base_channels

            # ËÆ°ÁÆóÂêÑÈò∂ÊÆµÈÄöÈÅìÊï∞
            self.encoder_channels = [
                2 * base_channels,  # Stage 1: 2C
                4 * base_channels,  # Stage 2: 4C
                8 * base_channels,  # Stage 3: 8C
                8 * base_channels,  # Stage 4: 8C
            ]

            # 1. Ê∑∑ÂêàÂºèÁºñÁ†ÅÂô®
            self.encoder = HybridEncoder(
                in_channels=in_channels,
                base_channels=base_channels,
                depths=encoder_depths,
                drop_path_rate=encoder_drop_path_rate,
                d_state=d_state,
            )

            # 2. HMAÁì∂È¢àÂ±Ç
            self.bottleneck = HMABottleneck(
                in_channels=self.encoder_channels[3],
                out_channels=self.encoder_channels[3],
                d_state=d_state,
                num_levels=bottleneck_num_levels,
                drop_path_rate=bottleneck_drop_path_rate,
                use_checkpoint=False,
            )

            # 3. ResNetËß£Á†ÅÂô®
            self.decoder = ResNetDecoder(
                base_channels=base_channels,
                encoder_channels=self.encoder_channels,
                reduction_ratio=csfg_reduction_ratio,
                use_transpose_conv=True,
                num_res_blocks=decoder_num_res_blocks,
            )

            # 4. ËæìÂá∫Â§¥
            self.output_head = OutputHead(
                in_channels=base_channels, num_classes=num_classes, dropout=dropout
            )

        def forward(self, x):
            # Á°Æ‰øùËæìÂÖ•Âú®CUDAËÆæÂ§á‰∏ä
            if not x.is_cuda:
                raise RuntimeError("Input tensor must be on CUDA device")
            
            input_size = x.shape[2:]

            # ÁºñÁ†ÅÂô®
            encoder_features = self.encoder(x)
            x_enc1, x_enc2, x_enc3, x_enc4 = encoder_features

            # StemÁâπÂæÅ
            x_stem = self.encoder.stem(x)

            # Áì∂È¢àÂ±Ç
            bottleneck_features = self.bottleneck(x_enc4)

            # Ëß£Á†ÅÂô®
            decoder_features = [x_enc1, x_enc2, x_enc3, x_stem]
            decoded_features = self.decoder(bottleneck_features, decoder_features)

            # ËæìÂá∫Â§¥
            output = self.output_head(decoded_features)

            # Á°Æ‰øùËæìÂá∫Â∞∫ÂØ∏ÂåπÈÖç
            if output.shape[2:] != input_size:
                output = torch.nn.functional.interpolate(
                    output, size=input_size, mode="bilinear", align_corners=True
                )

            return output

        def get_model_info(self):
            return {
                "model_name": "HMA-UNet",
                "input_channels": self.in_channels,
                "num_classes": self.num_classes,
                "base_channels": self.base_channels,
                "encoder_channels": self.encoder_channels,
                "total_params": sum(p.numel() for p in self.parameters()),
                "trainable_params": sum(
                    p.numel() for p in self.parameters() if p.requires_grad
                ),
            }

    return HMAUNetLocal()


def test_basic_components():
    """ÊµãËØïÂü∫Á°ÄÁªÑ‰ª∂ - CUDA only"""
    print("\n" + "=" * 80)
    print("Basic Components Test (CUDA Required)")
    print("=" * 80)

    # ËÆæÁΩÆCUDAËÆæÂ§á
    device = torch.device("cuda")
    print(f"Testing on device: {device}")

    try:
        # ÊµãËØïResNetÂùó
        print("üîß Testing ResNet blocks...")
        from models.backbones.ResNet_blocks import ResidualBlock

        res_block = ResidualBlock(64, 64).to(device)
        x = torch.randn(1, 64, 32, 32).to(device)
        with torch.no_grad():
            y = res_block(x)
        print(f"   ResidualBlock: {x.shape} -> {y.shape}")

        # ÊµãËØïConvNeXtÂùó
        print("üîß Testing ConvNeXt blocks...")
        from models.backbones.convnext_blocks import ConvNeXtV2Block

        convnext_block = ConvNeXtV2Block(64).to(device)
        x = torch.randn(1, 64, 32, 32).to(device)  # ConvNeXtÊé•Âèó(N,C,H,W)Ê†ºÂºè
        with torch.no_grad():
            y = convnext_block(x)
        print(f"   ConvNeXtV2Block: {x.shape} -> {y.shape}")

        # ÊµãËØïVSSÂùóÔºàÂÆòÊñπmambaÔºâ
        print("üîß Testing VSS blocks (Official Mamba)...")
        from models.backbones.vss_blocks import VSSBlock

        vss_block = VSSBlock(hidden_dim=64).to(device)
        x = torch.randn(1, 32, 32, 64).to(device)  # VSSÊúüÊúõ(N,H,W,C)Ê†ºÂºè
        with torch.no_grad():
            y = vss_block(x)
        print(f"   VSSBlock: {x.shape} -> {y.shape}")
        print(f"   Device: {x.device} -> {y.device}")

        print("   ‚úÖ Basic components test PASSED")

    except Exception as e:
        print(f"   ‚ùå Basic components test FAILED: {e}")
        traceback.print_exc()


def test_encoder_features():
    """ÊµãËØïÁºñÁ†ÅÂô®ÁâπÂæÅËæìÂá∫ - CUDA only"""
    print("\n" + "=" * 80)
    print("Encoder Feature Maps Test (CUDA Required)")
    print("=" * 80)

    device = torch.device("cuda")

    try:
        from models.components.encoder import HybridEncoder

        # ÂàõÂª∫ÁºñÁ†ÅÂô®Âπ∂ÁßªÂä®Âà∞CUDA
        encoder = HybridEncoder(in_channels=3, base_channels=32).to(device)
        encoder.eval()  # ËÆæÁΩÆ‰∏∫ËØÑ‰º∞Ê®°Âºè

        # ÊµãËØïËæìÂÖ•
        x = torch.randn(1, 3, 128, 128).to(device)
        print(f"Input shape: {x.shape} on {x.device}")

        # Ëé∑ÂèñÁºñÁ†ÅÂô®ÁâπÂæÅ
        print("\nüîç Testing encoder feature extraction...")
        with torch.no_grad():
            encoder_features = encoder(x)

        print("\nEncoder feature shapes:")
        for i, feature in enumerate(encoder_features):
            print(f"   Stage {i+1}: {feature.shape} on {feature.device}")

        # ÊµãËØïstemËæìÂá∫
        with torch.no_grad():
            stem_output = encoder.stem(x)
        print(f"   Stem output: {stem_output.shape} on {stem_output.device}")

        print("   ‚úÖ Encoder features test PASSED")

    except Exception as e:
        print(f"   ‚ùå Encoder features test FAILED: {e}")
        traceback.print_exc()


def test_bottleneck_module():
    """ÊµãËØïHMAÁì∂È¢àÂ±Ç - CUDA only"""
    print("\n" + "=" * 80)
    print("HMA Bottleneck Module Test (CUDA Required)")
    print("=" * 80)

    device = torch.device("cuda")

    try:
        from models.components.hma_module import HMABottleneck

        # ÂàõÂª∫Áì∂È¢àÂ±ÇÂπ∂ÁßªÂä®Âà∞CUDA
        bottleneck = HMABottleneck(in_channels=256, d_state=16).to(device)
        bottleneck.eval()  # ËÆæÁΩÆ‰∏∫ËØÑ‰º∞Ê®°Âºè

        # Ê®°ÊãüÁºñÁ†ÅÂô®ËæìÂá∫ - ‰øÆÊ≠£‰∏∫H/16
        x_enc4 = torch.randn(1, 256, 8, 8).to(device)  # H/16 = 128/16 = 8
        print(f"Bottleneck input shape: {x_enc4.shape} on {x_enc4.device}")

        # ÊµãËØïÁì∂È¢àÂ±Ç
        print("\nüîÑ Testing HMA bottleneck...")
        with torch.no_grad():
            bottleneck_output = bottleneck(x_enc4)

        print(f"Bottleneck output shape: {bottleneck_output.shape} on {bottleneck_output.device}")

        # È™åËØÅËæìÂá∫ÂΩ¢Áä∂
        assert (
            bottleneck_output.shape == x_enc4.shape
        ), f"Expected {x_enc4.shape}, got {bottleneck_output.shape}"

        # Ëé∑ÂèñÁâπÂæÅ‰ø°ÊÅØ
        feature_info = bottleneck.get_feature_info()
        print("\nBottleneck feature info:")
        for key, value in feature_info.items():
            print(f"   {key}: {value}")

        print("   ‚úÖ HMA bottleneck test PASSED")

    except Exception as e:
        print(f"   ‚ùå HMA bottleneck test FAILED: {e}")
        traceback.print_exc()


def test_csfg_module():
    """ÊµãËØïCSFGÊô∫ËÉΩË∑≥Ë∑ÉËøûÊé•Ê®°Âùó - CUDA only"""
    print("\n" + "=" * 80)
    print("CSFG Module Test (CUDA Required)")
    print("=" * 80)

    device = torch.device("cuda")

    try:
        from models.components.csfg_module import CSFGModule, CSFGSkipConnection

        # ÊµãËØïÂü∫Á°ÄCSFGÊ®°Âùó
        print("üîß Testing basic CSFG module...")
        csfg = CSFGModule(enc_channels=128, dec_channels=64).to(device)
        csfg.eval()  # ËÆæÁΩÆ‰∏∫ËØÑ‰º∞Ê®°Âºè

        x_enc = torch.randn(1, 128, 32, 32).to(device)
        g_up = torch.randn(1, 64, 32, 32).to(device)

        with torch.no_grad():
            x_fused = csfg(x_enc, g_up)
        print(f"   CSFG fusion: enc{x_enc.shape} + dec{g_up.shape} -> {x_fused.shape}")

        # È™åËØÅËæìÂá∫ÂΩ¢Áä∂
        assert x_fused.shape == x_enc.shape, f"Expected {x_enc.shape}, got {x_fused.shape}"

        # ÊµãËØïÂÆåÊï¥ÁöÑCSFGË∑≥Ë∑ÉËøûÊé•
        print("üîß Testing CSFG skip connection...")
        csfg_skip = CSFGSkipConnection(
            enc_channels=128, dec_channels=64, out_channels=96
        ).to(device)
        csfg_skip.eval()  # ËÆæÁΩÆ‰∏∫ËØÑ‰º∞Ê®°Âºè

        with torch.no_grad():
            skip_output = csfg_skip(x_enc, g_up)
        print(
            f"   CSFG skip: enc{x_enc.shape} + dec{g_up.shape} -> {skip_output.shape}"
        )

        # ÊµãËØïÊ≥®ÊÑèÂäõÊùÉÈáçËé∑Âèñ - ‰øÆÂ§çËøôÈÉ®ÂàÜ
        print("üîß Testing attention weights...")
        
        # Á¨¨‰∏ÄÊ≠•ÔºöÂºÇÊ≠•Ëé∑ÂèñÊùÉÈáç
        weights_async = csfg.get_attention_weights(x_enc, g_up)
        print(f"   Async weights device: {weights_async['device']}")
        
        # Á¨¨‰∫åÊ≠•ÔºöÂêåÊ≠•Ëé∑ÂèñÂÖ∑‰ΩìÊï∞ÂÄºÔºàÂè™Âú®ÈúÄË¶ÅÊó∂Ôºâ
        attention_weights = csfg.get_attention_weights_legacy(x_enc, g_up)  # ‰ΩøÁî®ÂÖºÂÆπÊñπÊ≥ï
        
        print(f"   Detail weight: {attention_weights['detail_weight'][0]:.3f}")
        print(f"   Local weight: {attention_weights['local_weight'][0]:.3f}")
        print(f"   Context weight: {attention_weights['context_weight'][0]:.3f}")

        # È™åËØÅÊùÉÈáçÂíå‰∏∫1ÔºàËΩØÁ∫¶ÊùüÔºåÂÖÅËÆ∏Â∞èËØØÂ∑ÆÔºâ
        total_weight = (attention_weights['detail_weight'][0] + 
                       attention_weights['local_weight'][0] + 
                       attention_weights['context_weight'][0])
        print(f"   Total weight: {total_weight:.3f} (should be ~1.0)")

        print("   ‚úÖ CSFG module test PASSED")

    except Exception as e:
        print(f"   ‚ùå CSFG module test FAILED: {e}")
        traceback.print_exc()

def test_decoder_module():
    """ÊµãËØïËß£Á†ÅÂô®Ê®°Âùó - CUDA only"""
    print("\n" + "=" * 80)
    print("Decoder Module Test (CUDA Required)")
    print("=" * 80)

    device = torch.device("cuda")

    try:
        from models.components.decoder import ResNetDecoder

        # ÂàõÂª∫Ëß£Á†ÅÂô®Âπ∂ÁßªÂä®Âà∞CUDA
        decoder = ResNetDecoder(base_channels=32).to(device)
        decoder.eval()  # ËÆæÁΩÆ‰∏∫ËØÑ‰º∞Ê®°Âºè

        # ‰øÆÊ≠£ËæìÂÖ•Â∞∫ÂØ∏ - Âü∫‰∫éÁºñÁ†ÅÂô®ÁöÑÂÆûÈôÖËæìÂá∫
        bottleneck_features = torch.randn(1, 256, 8, 8).to(device)   # HMAËæìÂá∫ (H/16)
        x_enc1 = torch.randn(1, 64, 32, 32).to(device)   # Stage 1 output (H/4)  
        x_enc2 = torch.randn(1, 128, 16, 16).to(device)  # Stage 2 output (H/8)
        x_enc3 = torch.randn(1, 256, 8, 8).to(device)    # Stage 3 output (H/16)
        x_stem = torch.randn(1, 32, 64, 64).to(device)   # Stem output (H/2)

        encoder_features = [x_enc1, x_enc2, x_enc3, x_stem]

        print("Decoder input shapes:")
        print(f"   Bottleneck: {bottleneck_features.shape} on {bottleneck_features.device}")
        for i, feat in enumerate(encoder_features):
            stage_name = ["enc1", "enc2", "enc3", "stem"][i]
            print(f"   {stage_name}: {feat.shape} on {feat.device}")

        # ÊµãËØïËß£Á†ÅÂô®
        print("\nüîÑ Testing decoder...")
        with torch.no_grad():
            decoder_output = decoder(bottleneck_features, encoder_features)

        print(f"Decoder output shape: {decoder_output.shape} on {decoder_output.device}")

        # Ëé∑ÂèñËß£Á†ÅÂô®ÂêÑÈò∂ÊÆµÈÄöÈÅì‰ø°ÊÅØ
        decoder_channels = decoder.get_feature_channels()
        print("\nDecoder stage channels:")
        for stage, channels in decoder_channels.items():
            print(f"   {stage}: {channels}")

        print("   ‚úÖ Decoder test PASSED")

    except Exception as e:
        print(f"   ‚ùå Decoder test FAILED: {e}")
        traceback.print_exc()


def test_model_architecture():
    """ÊµãËØïÊ®°ÂûãÊû∂ÊûÑÂíåÂâçÂêë‰º†Êí≠ - CUDA only"""
    print("\n" + "=" * 80)
    print("HMA-UNet Model Architecture Test (CUDA Required)")
    print("=" * 80)

    device = torch.device("cuda")

    # ÊµãËØïÈÖçÁΩÆ - ‰ΩøÁî®Êõ¥Â∞èÁöÑÂ∞∫ÂØ∏ÂíåÁÆÄÂåñÈÖçÁΩÆ
    configs = {
        "tiny": {
            "input_size": (128, 128),  # ‰ΩøÁî®128x128ÈÅøÂÖçÂ∞∫ÂØ∏ÈóÆÈ¢ò
            "batch_size": 1,
            "expected_params": "~2-3M",
        },
    }

    for config_name, config in configs.items():
        print(f"\nüß™ Testing {config_name.upper()} configuration...")
        try:
            # ÂàõÂª∫Ê®°ÂûãÂπ∂ÁßªÂä®Âà∞CUDA
            model = create_hma_unet_local(
                config=config_name, in_channels=3, num_classes=1
            ).to(device)
            model.eval()  # ËÆæÁΩÆ‰∏∫ËØÑ‰º∞Ê®°Âºè

            # ÂàõÂª∫ÊµãËØïËæìÂÖ•
            batch_size = config["batch_size"]
            input_size = config["input_size"]
            x = torch.randn(batch_size, 3, *input_size).to(device)

            print(f"   Input shape: {x.shape} on {x.device}")

            # Ê®°Âûã‰ø°ÊÅØ
            model_info = model.get_model_info()
            total_params = model_info["total_params"]
            print(f"   Total parameters: {total_params:,}")
            print(f"   Expected: {config['expected_params']}")
            print(f"   Base channels: {model_info['base_channels']}")

            # ÂâçÂêë‰º†Êí≠ÊµãËØï
            print("   üîÑ Testing forward pass...")
            with torch.no_grad():
                start_time = time.time()
                output = model(x)
                forward_time = time.time() - start_time

            print(f"   Output shape: {output.shape} on {output.device}")
            print(f"   Forward time: {forward_time:.4f}s")

            # È™åËØÅËæìÂá∫ÂΩ¢Áä∂
            expected_shape = (batch_size, 1, *input_size)
            assert (
                output.shape == expected_shape
            ), f"Expected {expected_shape}, got {output.shape}"

            # È™åËØÅËæìÂá∫ÂÄºËåÉÂõ¥
            output_min, output_max = output.min().item(), output.max().item()
            print(f"   Output range: [{output_min:.4f}, {output_max:.4f}]")

            print(f"   ‚úÖ {config_name.upper()} configuration test PASSED")

        except Exception as e:
            print(f"   ‚ùå {config_name.upper()} configuration test FAILED: {e}")
            traceback.print_exc()


def test_model_gradient_flow():
    """ÊµãËØïÊ®°ÂûãÊ¢ØÂ∫¶ÊµÅÂä® - CUDA only"""
    print("\n" + "=" * 80)
    print("Gradient Flow Test (CUDA Required)")
    print("=" * 80)

    device = torch.device("cuda")

    try:
        # ÂàõÂª∫Ê®°ÂûãÂπ∂ÁßªÂä®Âà∞CUDA
        model = create_hma_unet_local(config="tiny", in_channels=3, num_classes=1).to(device)
        model.train()

        # ÂàõÂª∫ÊµãËØïÊï∞ÊçÆ
        x = torch.randn(1, 3, 128, 128, requires_grad=True).to(device)
        y_true = torch.randint(0, 2, (1, 1, 128, 128)).float().to(device)

        print(f"Input shape: {x.shape} on {x.device}")
        print(f"Target shape: {y_true.shape} on {y_true.device}")

        # ÂâçÂêë‰º†Êí≠
        y_pred = model(x)

        # ËÆ°ÁÆóÁÆÄÂçïÊçüÂ§±
        loss = torch.nn.functional.binary_cross_entropy_with_logits(y_pred, y_true)

        # ÂèçÂêë‰º†Êí≠
        loss.backward()

        # Ê£ÄÊü•Ê¢ØÂ∫¶
        has_grad = 0
        total_params = 0
        for name, param in model.named_parameters():
            if param.requires_grad:
                total_params += 1
                if param.grad is not None:
                    has_grad += 1

        print(f"   Total parameters requiring gradients: {total_params}")
        print(f"   Parameters with gradients: {has_grad}")
        print(f"   Gradient coverage: {has_grad/total_params*100:.1f}%")
        print(f"   Loss value: {loss.item():.4f}")

        assert has_grad > 0, "No gradients computed!"
        print("   ‚úÖ Gradient flow test PASSED")

    except Exception as e:
        print(f"   ‚ùå Gradient flow test FAILED: {e}")
        traceback.print_exc()


def test_data_flow_consistency():
    """ÊµãËØïÊï∞ÊçÆÊµÅ‰∏ÄËá¥ÊÄß - CUDA only"""
    print("\n" + "=" * 80)
    print("Data Flow Consistency Test (CUDA Required)")
    print("=" * 80)

    device = torch.device("cuda")

    try:
        # ÂàõÂª∫Ê®°ÂûãÂπ∂ÁßªÂä®Âà∞CUDA
        model = create_hma_unet_local(config="tiny", in_channels=3, num_classes=1).to(device)
        model.eval()

        # ÊµãËØïËæìÂÖ•
        x = torch.randn(1, 3, 128, 128).to(device)
        print(f"Input: {x.shape} on {x.device}")

        # ÈÄêÊ≠•ÊµãËØïÊï∞ÊçÆÊµÅ
        print("\nüîç Testing step-by-step data flow...")

        with torch.no_grad():
            # 1. StemÂ±Ç
            stem_out = model.encoder.stem(x)
            print(f"   Stem output: {stem_out.shape} on {stem_out.device}")

            # 2. ÁºñÁ†ÅÂô®ÂêÑÈò∂ÊÆµ
            enc_features = model.encoder(x)
            for i, feat in enumerate(enc_features):
                print(f"   Encoder stage {i+1}: {feat.shape} on {feat.device}")

            # 3. Áì∂È¢àÂ±Ç
            bottleneck_out = model.bottleneck(enc_features[-1])
            print(f"   Bottleneck output: {bottleneck_out.shape} on {bottleneck_out.device}")

            # 4. Ëß£Á†ÅÂô®
            decoder_features = [
                enc_features[0],
                enc_features[1],
                enc_features[2],
                stem_out,
            ]
            decoded_out = model.decoder(bottleneck_out, decoder_features)
            print(f"   Decoder output: {decoded_out.shape} on {decoded_out.device}")

            # 5. ËæìÂá∫Â§¥
            final_out = model.output_head(decoded_out)
            print(f"   Final output: {final_out.shape} on {final_out.device}")

            # 6. ÂÆåÊï¥ÂâçÂêë‰º†Êí≠
            complete_out = model(x)
            print(f"   Complete forward: {complete_out.shape} on {complete_out.device}")

            # È™åËØÅ‰∏ÄËá¥ÊÄß
            assert torch.allclose(
                final_out, complete_out, atol=1e-6
            ), "Output mismatch!"

        print("   ‚úÖ Data flow consistency test PASSED")

    except Exception as e:
        print(f"   ‚ùå Data flow consistency test FAILED: {e}")
        traceback.print_exc()


def test_memory_usage():
    """ÊµãËØïÂÜÖÂ≠ò‰ΩøÁî®ÊÉÖÂÜµ - CUDA only"""
    print("\n" + "=" * 80)
    print("Memory Usage Test (CUDA Required)")
    print("=" * 80)

    device = torch.device("cuda")

    try:
        import psutil
        import os

        process = psutil.Process(os.getpid())

        # ‰∏çÂêåÈÖçÁΩÆÁöÑÂÜÖÂ≠òÊµãËØï
        configs = ["tiny"]

        for config in configs:
            print(f"\nüíæ Testing {config} configuration memory usage...")

            # ËÆ∞ÂΩïÂàùÂßãÂÜÖÂ≠ò
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB
            initial_gpu_memory = torch.cuda.memory_allocated(device) / 1024 / 1024  # MB

            # ÂàõÂª∫Ê®°ÂûãÂπ∂ÁßªÂä®Âà∞CUDA
            model = create_hma_unet_local(config=config, in_channels=3, num_classes=1).to(device)
            model.eval()  # ËÆæÁΩÆ‰∏∫ËØÑ‰º∞Ê®°Âºè

            # ËÆ∞ÂΩïÊ®°ÂûãÂä†ËΩΩÂêéÂÜÖÂ≠ò
            model_memory = process.memory_info().rss / 1024 / 1024  # MB
            model_gpu_memory = torch.cuda.memory_allocated(device) / 1024 / 1024  # MB

            # ÂâçÂêë‰º†Êí≠
            x = torch.randn(1, 3, 128, 128).to(device)
            with torch.no_grad():
                output = model(x)

            # ËÆ∞ÂΩïÂâçÂêë‰º†Êí≠ÂêéÂÜÖÂ≠ò
            forward_memory = process.memory_info().rss / 1024 / 1024  # MB
            forward_gpu_memory = torch.cuda.memory_allocated(device) / 1024 / 1024  # MB

            print(f"   CPU Memory:")
            print(f"     Initial: {initial_memory:.1f} MB")
            print(f"     After model loading: {model_memory:.1f} MB (+{model_memory-initial_memory:.1f} MB)")
            print(f"     After forward pass: {forward_memory:.1f} MB (+{forward_memory-model_memory:.1f} MB)")
            
            print(f"   GPU Memory:")
            print(f"     Initial: {initial_gpu_memory:.1f} MB")
            print(f"     After model loading: {model_gpu_memory:.1f} MB (+{model_gpu_memory-initial_gpu_memory:.1f} MB)")
            print(f"     After forward pass: {forward_gpu_memory:.1f} MB (+{forward_gpu_memory-model_gpu_memory:.1f} MB)")

            # Ê∏ÖÁêÜ
            del model, x, output
            torch.cuda.empty_cache()

        print("   ‚úÖ Memory usage test PASSED")

    except ImportError:
        print("   ‚ö†Ô∏è  psutil not available, skipping memory test")
    except Exception as e:
        print(f"   ‚ùå Memory usage test FAILED: {e}")


def main():
    """‰∏ªÊµãËØïÂáΩÊï∞ - CUDA only"""
    print("üöÄ Starting HMA-UNet Model Tests (CUDA Required)...\n")
    
    # Âº∫Âà∂Ê£ÄÊü•CUDAÂèØÁî®ÊÄß
    check_cuda_availability()
    
    # ËÆæÁΩÆCUDAËÆæÂ§á
    torch.cuda.set_device(0)
    print(f"Using CUDA device: {torch.cuda.current_device()}")

    # ËÆæÁΩÆÈöèÊú∫ÁßçÂ≠ê
    torch.manual_seed(42)
    torch.cuda.manual_seed(42)

    # È¶ñÂÖàÊµãËØïÂØºÂÖ•
    if not test_imports():
        print("‚ùå Import test failed. Cannot proceed with other tests.")
        return

    # ËøêË°åÂÖ∂‰ªñÊµãËØï
    test_basic_components()
    test_encoder_features()
    test_csfg_module()
    test_bottleneck_module()
    test_decoder_module()
    test_model_architecture()
    test_model_gradient_flow()
    test_data_flow_consistency()
    test_memory_usage()

    print("\n" + "=" * 80)
    print("üéâ HMA-UNet tests completed!")
    print("=" * 80)


if __name__ == "__main__":
    main()
import torch
import torch.nn as nn
import traceback
import time
import sys
import os

# ç¡®ä¿ä»æ­£ç¡®çš„ç›®å½•è¿è¡Œ
if os.getcwd().endswith("models"):
    os.chdir("..")  # åˆ‡æ¢åˆ°é¡¹ç›®æ ¹ç›®å½•

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.getcwd()
if project_root not in sys.path:
    sys.path.insert(0, project_root)

print("Current working directory:", os.getcwd())
print("Python path:", sys.path[:3])

# å¼ºåˆ¶æ£€æŸ¥CUDAå¯ç”¨æ€§
def check_cuda_availability():
    """æ£€æŸ¥CUDAå¯ç”¨æ€§ï¼Œå¦‚æœä¸å¯ç”¨åˆ™é€€å‡º"""
    if not torch.cuda.is_available():
        print("âŒ CUDA is not available!")
        print("âŒ This test suite requires CUDA for Mamba support.")
        print("âŒ Please ensure:")
        print("   1. NVIDIA GPU is available")
        print("   2. CUDA toolkit is installed")
        print("   3. PyTorch with CUDA support is installed")
        sys.exit(1)
    
    print(f"âœ… CUDA is available: {torch.cuda.get_device_name(0)}")
    print(f"âœ… CUDA version: {torch.version.cuda}")
    return True

def test_imports():
    """æµ‹è¯•å¯¼å…¥æ˜¯å¦æ­£å¸¸å·¥ä½œ"""
    print("=" * 80)
    print("Import Test")
    print("=" * 80)

    # é¦–å…ˆæ£€æŸ¥CUDA
    check_cuda_availability()

    try:
        # æµ‹è¯•å„ä¸ªç»„ä»¶çš„å¯¼å…¥
        print("ğŸ§© Testing component imports...")

        # é¦–å…ˆæµ‹è¯•åŸºç¡€æ¨¡å—
        try:
            from models.backbones.ResNet_blocks import (
                ResidualBlock,
                ConvBlock,
                TransposeConvBlock,
            )
            print("   âœ… ResNet blocks imported successfully")
        except Exception as e:
            print(f"   âŒ ResNet blocks import failed: {e}")
            return False

        try:
            from models.backbones.convnext_blocks import ConvNeXtV2Block
            print("   âœ… ConvNeXt blocks imported successfully")
        except Exception as e:
            print(f"   âŒ ConvNeXt blocks import failed: {e}")
            return False

        try:
            from models.backbones.vss_blocks import VSSBlock
            print("   âœ… VSS blocks imported successfully")
        except Exception as e:
            print(f"   âŒ VSS blocks import failed: {e}")
            return False

        try:
            from models.components.csfg_module import CSFGModule
            print("   âœ… CSFGModule imported successfully")
        except Exception as e:
            print(f"   âŒ CSFGModule import failed: {e}")
            return False

        try:
            from models.components.encoder import HybridEncoder
            print("   âœ… HybridEncoder imported successfully")
        except Exception as e:
            print(f"   âŒ HybridEncoder import failed: {e}")
            return False

        try:
            from models.components.hma_module import HMABottleneck
            print("   âœ… HMABottleneck imported successfully")
        except Exception as e:
            print(f"   âŒ HMABottleneck import failed: {e}")
            return False

        try:
            from models.components.decoder import ResNetDecoder, OutputHead
            print("   âœ… ResNetDecoder imported successfully")
        except Exception as e:
            print(f"   âŒ ResNetDecoder import failed: {e}")
            return False

        print("   âœ… All component imports test PASSED")
        return True

    except Exception as e:
        print(f"   âŒ Components test FAILED: {e}")
        traceback.print_exc()
        return False


def create_hma_unet_local(config="tiny", in_channels=3, num_classes=1, **kwargs):
    """æœ¬åœ°åˆ›å»ºHMA-UNetæ¨¡å‹çš„å‡½æ•° - CUDA only"""
    from models.components.encoder import HybridEncoder
    from models.components.hma_module import HMABottleneck
    from models.components.decoder import ResNetDecoder, OutputHead

    # æ ¹æ®é…ç½®è®¾ç½®å‚æ•°
    if config == "tiny":
        base_channels = 32
        encoder_depths = [2, 2, 2, 2]
        encoder_drop_path_rate = 0.1
        bottleneck_num_levels = 3
        bottleneck_drop_path_rate = 0.15
        csfg_reduction_ratio = 8
        decoder_num_res_blocks = 2
        d_state = 16
        dropout = 0.1
    elif config == "small":
        base_channels = 48
        encoder_depths = [2, 2, 4, 2]
        encoder_drop_path_rate = 0.15
        bottleneck_num_levels = 3
        bottleneck_drop_path_rate = 0.2
        csfg_reduction_ratio = 6
        decoder_num_res_blocks = 3
        d_state = 16
        dropout = 0.1
    elif config == "base":
        base_channels = 64
        encoder_depths = [3, 3, 6, 3]
        encoder_drop_path_rate = 0.2
        bottleneck_num_levels = 4
        bottleneck_drop_path_rate = 0.25
        csfg_reduction_ratio = 4
        decoder_num_res_blocks = 3
        d_state = 24
        dropout = 0.1
    else:
        raise ValueError(f"Unknown config: {config}")

    # åº”ç”¨é¢å¤–å‚æ•°
    for key, value in kwargs.items():
        locals()[key] = value

    class HMAUNetLocal(nn.Module):
        def __init__(self):
            super().__init__()

            self.in_channels = in_channels
            self.num_classes = num_classes
            self.base_channels = base_channels

            # è®¡ç®—å„é˜¶æ®µé€šé“æ•°
            self.encoder_channels = [
                2 * base_channels,  # Stage 1: 2C
                4 * base_channels,  # Stage 2: 4C
                8 * base_channels,  # Stage 3: 8C
                8 * base_channels,  # Stage 4: 8C
            ]

            # 1. æ··åˆå¼ç¼–ç å™¨
            self.encoder = HybridEncoder(
                in_channels=in_channels,
                base_channels=base_channels,
                depths=encoder_depths,
                drop_path_rate=encoder_drop_path_rate,
                d_state=d_state,
            )

            # 2. HMAç“¶é¢ˆå±‚
            self.bottleneck = HMABottleneck(
                in_channels=self.encoder_channels[3],
                out_channels=self.encoder_channels[3],
                d_state=d_state,
                num_levels=bottleneck_num_levels,
                drop_path_rate=bottleneck_drop_path_rate,
                use_checkpoint=False,
            )

            # 3. ResNetè§£ç å™¨
            self.decoder = ResNetDecoder(
                base_channels=base_channels,
                encoder_channels=self.encoder_channels,
                reduction_ratio=csfg_reduction_ratio,
                use_transpose_conv=True,
                num_res_blocks=decoder_num_res_blocks,
            )

            # 4. è¾“å‡ºå¤´
            self.output_head = OutputHead(
                in_channels=base_channels, num_classes=num_classes, dropout=dropout
            )

        def forward(self, x):
            # ç¡®ä¿è¾“å…¥åœ¨CUDAè®¾å¤‡ä¸Š
            if not x.is_cuda:
                raise RuntimeError("Input tensor must be on CUDA device")
            
            input_size = x.shape[2:]

            # ç¼–ç å™¨
            encoder_features = self.encoder(x)
            x_enc1, x_enc2, x_enc3, x_enc4 = encoder_features

            # Stemç‰¹å¾
            x_stem = self.encoder.stem(x)

            # ç“¶é¢ˆå±‚
            bottleneck_features = self.bottleneck(x_enc4)

            # è§£ç å™¨
            decoder_features = [x_enc1, x_enc2, x_enc3, x_stem]
            decoded_features = self.decoder(bottleneck_features, decoder_features)

            # è¾“å‡ºå¤´
            output = self.output_head(decoded_features)

            # ç¡®ä¿è¾“å‡ºå°ºå¯¸åŒ¹é…
            if output.shape[2:] != input_size:
                output = torch.nn.functional.interpolate(
                    output, size=input_size, mode="bilinear", align_corners=True
                )

            return output

        def get_model_info(self):
            return {
                "model_name": "HMA-UNet",
                "input_channels": self.in_channels,
                "num_classes": self.num_classes,
                "base_channels": self.base_channels,
                "encoder_channels": self.encoder_channels,
                "total_params": sum(p.numel() for p in self.parameters()),
                "trainable_params": sum(
                    p.numel() for p in self.parameters() if p.requires_grad
                ),
            }

    return HMAUNetLocal()


def test_basic_components():
    """æµ‹è¯•åŸºç¡€ç»„ä»¶ - CUDA only"""
    print("\n" + "=" * 80)
    print("Basic Components Test (CUDA Required)")
    print("=" * 80)

    # è®¾ç½®CUDAè®¾å¤‡
    device = torch.device("cuda")
    print(f"Testing on device: {device}")

    try:
        # æµ‹è¯•ResNetå—
        print("ğŸ”§ Testing ResNet blocks...")
        from models.backbones.ResNet_blocks import ResidualBlock

        res_block = ResidualBlock(64, 64).to(device)
        x = torch.randn(1, 64, 32, 32).to(device)
        with torch.no_grad():
            y = res_block(x)
        print(f"   ResidualBlock: {x.shape} -> {y.shape}")

        # æµ‹è¯•ConvNeXtå—
        print("ğŸ”§ Testing ConvNeXt blocks...")
        from models.backbones.convnext_blocks import ConvNeXtV2Block

        convnext_block = ConvNeXtV2Block(64).to(device)
        x = torch.randn(1, 64, 32, 32).to(device)  # ConvNeXtæ¥å—(N,C,H,W)æ ¼å¼
        with torch.no_grad():
            y = convnext_block(x)
        print(f"   ConvNeXtV2Block: {x.shape} -> {y.shape}")

        # æµ‹è¯•VSSå—ï¼ˆå®˜æ–¹mambaï¼‰
        print("ğŸ”§ Testing VSS blocks (Official Mamba)...")
        from models.backbones.vss_blocks import VSSBlock

        vss_block = VSSBlock(hidden_dim=64).to(device)
        x = torch.randn(1, 32, 32, 64).to(device)  # VSSæœŸæœ›(N,H,W,C)æ ¼å¼
        with torch.no_grad():
            y = vss_block(x)
        print(f"   VSSBlock: {x.shape} -> {y.shape}")
        print(f"   Device: {x.device} -> {y.device}")

        print("   âœ… Basic components test PASSED")

    except Exception as e:
        print(f"   âŒ Basic components test FAILED: {e}")
        traceback.print_exc()


def test_encoder_features():
    """æµ‹è¯•ç¼–ç å™¨ç‰¹å¾è¾“å‡º - CUDA only"""
    print("\n" + "=" * 80)
    print("Encoder Feature Maps Test (CUDA Required)")
    print("=" * 80)

    device = torch.device("cuda")

    try:
        from models.components.encoder import HybridEncoder

        # åˆ›å»ºç¼–ç å™¨å¹¶ç§»åŠ¨åˆ°CUDA
        encoder = HybridEncoder(in_channels=3, base_channels=32).to(device)
        encoder.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

        # æµ‹è¯•è¾“å…¥
        x = torch.randn(1, 3, 128, 128).to(device)
        print(f"Input shape: {x.shape} on {x.device}")

        # è·å–ç¼–ç å™¨ç‰¹å¾
        print("\nğŸ” Testing encoder feature extraction...")
        with torch.no_grad():
            encoder_features = encoder(x)

        print("\nEncoder feature shapes:")
        for i, feature in enumerate(encoder_features):
            print(f"   Stage {i+1}: {feature.shape} on {feature.device}")

        # æµ‹è¯•stemè¾“å‡º
        with torch.no_grad():
            stem_output = encoder.stem(x)
        print(f"   Stem output: {stem_output.shape} on {stem_output.device}")

        print("   âœ… Encoder features test PASSED")

    except Exception as e:
        print(f"   âŒ Encoder features test FAILED: {e}")
        traceback.print_exc()


def test_bottleneck_module():
    """æµ‹è¯•HMAç“¶é¢ˆå±‚ - CUDA only"""
    print("\n" + "=" * 80)
    print("HMA Bottleneck Module Test (CUDA Required)")
    print("=" * 80)

    device = torch.device("cuda")

    try:
        from models.components.hma_module import HMABottleneck

        # åˆ›å»ºç“¶é¢ˆå±‚å¹¶ç§»åŠ¨åˆ°CUDA
        bottleneck = HMABottleneck(in_channels=256, d_state=16).to(device)
        bottleneck.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

        # æ¨¡æ‹Ÿç¼–ç å™¨è¾“å‡º - ä¿®æ­£ä¸ºH/16
        x_enc4 = torch.randn(1, 256, 8, 8).to(device)  # H/16 = 128/16 = 8
        print(f"Bottleneck input shape: {x_enc4.shape} on {x_enc4.device}")

        # æµ‹è¯•ç“¶é¢ˆå±‚
        print("\nğŸ”„ Testing HMA bottleneck...")
        with torch.no_grad():
            bottleneck_output = bottleneck(x_enc4)

        print(f"Bottleneck output shape: {bottleneck_output.shape} on {bottleneck_output.device}")

        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        assert (
            bottleneck_output.shape == x_enc4.shape
        ), f"Expected {x_enc4.shape}, got {bottleneck_output.shape}"

        # è·å–ç‰¹å¾ä¿¡æ¯
        feature_info = bottleneck.get_feature_info()
        print("\nBottleneck feature info:")
        for key, value in feature_info.items():
            print(f"   {key}: {value}")

        print("   âœ… HMA bottleneck test PASSED")

    except Exception as e:
        print(f"   âŒ HMA bottleneck test FAILED: {e}")
        traceback.print_exc()


def test_csfg_module():
    """æµ‹è¯•CSFGæ™ºèƒ½è·³è·ƒè¿æ¥æ¨¡å— - CUDA only"""
    print("\n" + "=" * 80)
    print("CSFG Module Test (CUDA Required)")
    print("=" * 80)

    device = torch.device("cuda")

    try:
        from models.components.csfg_module import CSFGModule, CSFGSkipConnection

        # æµ‹è¯•åŸºç¡€CSFGæ¨¡å—
        print("ğŸ”§ Testing basic CSFG module...")
        csfg = CSFGModule(enc_channels=128, dec_channels=64).to(device)
        csfg.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

        x_enc = torch.randn(1, 128, 32, 32).to(device)
        g_up = torch.randn(1, 64, 32, 32).to(device)

        with torch.no_grad():
            x_fused = csfg(x_enc, g_up)
        print(f"   CSFG fusion: enc{x_enc.shape} + dec{g_up.shape} -> {x_fused.shape}")

        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        assert x_fused.shape == x_enc.shape, f"Expected {x_enc.shape}, got {x_fused.shape}"

        # æµ‹è¯•å®Œæ•´çš„CSFGè·³è·ƒè¿æ¥
        print("ğŸ”§ Testing CSFG skip connection...")
        csfg_skip = CSFGSkipConnection(
            enc_channels=128, dec_channels=64, out_channels=96
        ).to(device)
        csfg_skip.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

        with torch.no_grad():
            skip_output = csfg_skip(x_enc, g_up)
        print(
            f"   CSFG skip: enc{x_enc.shape} + dec{g_up.shape} -> {skip_output.shape}"
        )

        # æµ‹è¯•æ³¨æ„åŠ›æƒé‡è·å– - ä¿®å¤è¿™éƒ¨åˆ†
        print("ğŸ”§ Testing attention weights...")
        
        # ç¬¬ä¸€æ­¥ï¼šå¼‚æ­¥è·å–æƒé‡
        weights_async = csfg.get_attention_weights(x_enc, g_up)
        print(f"   Async weights device: {weights_async['device']}")
        
        # ç¬¬äºŒæ­¥ï¼šåŒæ­¥è·å–å…·ä½“æ•°å€¼ï¼ˆåªåœ¨éœ€è¦æ—¶ï¼‰
        attention_weights = csfg.get_attention_weights_legacy(x_enc, g_up)  # ä½¿ç”¨å…¼å®¹æ–¹æ³•
        
        print(f"   Detail weight: {attention_weights['detail_weight'][0]:.3f}")
        print(f"   Local weight: {attention_weights['local_weight'][0]:.3f}")
        print(f"   Context weight: {attention_weights['context_weight'][0]:.3f}")

        # éªŒè¯æƒé‡å’Œä¸º1ï¼ˆè½¯çº¦æŸï¼Œå…è®¸å°è¯¯å·®ï¼‰
        total_weight = (attention_weights['detail_weight'][0] + 
                       attention_weights['local_weight'][0] + 
                       attention_weights['context_weight'][0])
        print(f"   Total weight: {total_weight:.3f} (should be ~1.0)")

        print("   âœ… CSFG module test PASSED")

    except Exception as e:
        print(f"   âŒ CSFG module test FAILED: {e}")
        traceback.print_exc()

def test_decoder_module():
    """æµ‹è¯•è§£ç å™¨æ¨¡å— - CUDA only"""
    print("\n" + "=" * 80)
    print("Decoder Module Test (CUDA Required)")
    print("=" * 80)

    device = torch.device("cuda")

    try:
        from models.components.decoder import ResNetDecoder

        # åˆ›å»ºè§£ç å™¨å¹¶ç§»åŠ¨åˆ°CUDA
        decoder = ResNetDecoder(base_channels=32).to(device)
        decoder.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

        # ä¿®æ­£è¾“å…¥å°ºå¯¸ - åŸºäºç¼–ç å™¨çš„å®é™…è¾“å‡º
        bottleneck_features = torch.randn(1, 256, 8, 8).to(device)   # HMAè¾“å‡º (H/16)
        x_enc1 = torch.randn(1, 64, 32, 32).to(device)   # Stage 1 output (H/4)  
        x_enc2 = torch.randn(1, 128, 16, 16).to(device)  # Stage 2 output (H/8)
        x_enc3 = torch.randn(1, 256, 8, 8).to(device)    # Stage 3 output (H/16)
        x_stem = torch.randn(1, 32, 64, 64).to(device)   # Stem output (H/2)

        encoder_features = [x_enc1, x_enc2, x_enc3, x_stem]

        print("Decoder input shapes:")
        print(f"   Bottleneck: {bottleneck_features.shape} on {bottleneck_features.device}")
        for i, feat in enumerate(encoder_features):
            stage_name = ["enc1", "enc2", "enc3", "stem"][i]
            print(f"   {stage_name}: {feat.shape} on {feat.device}")

        # æµ‹è¯•è§£ç å™¨
        print("\nğŸ”„ Testing decoder...")
        with torch.no_grad():
            decoder_output = decoder(bottleneck_features, encoder_features)

        print(f"Decoder output shape: {decoder_output.shape} on {decoder_output.device}")

        # è·å–è§£ç å™¨å„é˜¶æ®µé€šé“ä¿¡æ¯
        decoder_channels = decoder.get_feature_channels()
        print("\nDecoder stage channels:")
        for stage, channels in decoder_channels.items():
            print(f"   {stage}: {channels}")

        print("   âœ… Decoder test PASSED")

    except Exception as e:
        print(f"   âŒ Decoder test FAILED: {e}")
        traceback.print_exc()


def test_model_architecture():
    """æµ‹è¯•æ¨¡å‹æ¶æ„å’Œå‰å‘ä¼ æ’­ - CUDA only"""
    print("\n" + "=" * 80)
    print("HMA-UNet Model Architecture Test (CUDA Required)")
    print("=" * 80)

    device = torch.device("cuda")

    # æµ‹è¯•é…ç½® - ä½¿ç”¨æ›´å°çš„å°ºå¯¸å’Œç®€åŒ–é…ç½®
    configs = {
        "tiny": {
            "input_size": (128, 128),  # ä½¿ç”¨128x128é¿å…å°ºå¯¸é—®é¢˜
            "batch_size": 1,
            "expected_params": "~2-3M",
        },
    }

    for config_name, config in configs.items():
        print(f"\nğŸ§ª Testing {config_name.upper()} configuration...")
        try:
            # åˆ›å»ºæ¨¡å‹å¹¶ç§»åŠ¨åˆ°CUDA
            model = create_hma_unet_local(
                config=config_name, in_channels=3, num_classes=1
            ).to(device)
            model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

            # åˆ›å»ºæµ‹è¯•è¾“å…¥
            batch_size = config["batch_size"]
            input_size = config["input_size"]
            x = torch.randn(batch_size, 3, *input_size).to(device)

            print(f"   Input shape: {x.shape} on {x.device}")

            # æ¨¡å‹ä¿¡æ¯
            model_info = model.get_model_info()
            total_params = model_info["total_params"]
            print(f"   Total parameters: {total_params:,}")
            print(f"   Expected: {config['expected_params']}")
            print(f"   Base channels: {model_info['base_channels']}")

            # å‰å‘ä¼ æ’­æµ‹è¯•
            print("   ğŸ”„ Testing forward pass...")
            with torch.no_grad():
                start_time = time.time()
                output = model(x)
                forward_time = time.time() - start_time

            print(f"   Output shape: {output.shape} on {output.device}")
            print(f"   Forward time: {forward_time:.4f}s")

            # éªŒè¯è¾“å‡ºå½¢çŠ¶
            expected_shape = (batch_size, 1, *input_size)
            assert (
                output.shape == expected_shape
            ), f"Expected {expected_shape}, got {output.shape}"

            # éªŒè¯è¾“å‡ºå€¼èŒƒå›´
            output_min, output_max = output.min().item(), output.max().item()
            print(f"   Output range: [{output_min:.4f}, {output_max:.4f}]")

            print(f"   âœ… {config_name.upper()} configuration test PASSED")

        except Exception as e:
            print(f"   âŒ {config_name.upper()} configuration test FAILED: {e}")
            traceback.print_exc()


def test_model_gradient_flow():
    """æµ‹è¯•æ¨¡å‹æ¢¯åº¦æµåŠ¨ - CUDA only"""
    print("\n" + "=" * 80)
    print("Gradient Flow Test (CUDA Required)")
    print("=" * 80)

    device = torch.device("cuda")

    try:
        # åˆ›å»ºæ¨¡å‹å¹¶ç§»åŠ¨åˆ°CUDA
        model = create_hma_unet_local(config="tiny", in_channels=3, num_classes=1).to(device)
        model.train()

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        x = torch.randn(1, 3, 128, 128, requires_grad=True).to(device)
        y_true = torch.randint(0, 2, (1, 1, 128, 128)).float().to(device)

        print(f"Input shape: {x.shape} on {x.device}")
        print(f"Target shape: {y_true.shape} on {y_true.device}")

        # å‰å‘ä¼ æ’­
        y_pred = model(x)

        # è®¡ç®—ç®€å•æŸå¤±
        loss = torch.nn.functional.binary_cross_entropy_with_logits(y_pred, y_true)

        # åå‘ä¼ æ’­
        loss.backward()

        # æ£€æŸ¥æ¢¯åº¦
        has_grad = 0
        total_params = 0
        for name, param in model.named_parameters():
            if param.requires_grad:
                total_params += 1
                if param.grad is not None:
                    has_grad += 1

        print(f"   Total parameters requiring gradients: {total_params}")
        print(f"   Parameters with gradients: {has_grad}")
        print(f"   Gradient coverage: {has_grad/total_params*100:.1f}%")
        print(f"   Loss value: {loss.item():.4f}")

        assert has_grad > 0, "No gradients computed!"
        print("   âœ… Gradient flow test PASSED")

    except Exception as e:
        print(f"   âŒ Gradient flow test FAILED: {e}")
        traceback.print_exc()


def test_data_flow_consistency():
    """æµ‹è¯•æ•°æ®æµä¸€è‡´æ€§ - CUDA only"""
    print("\n" + "=" * 80)
    print("Data Flow Consistency Test (CUDA Required)")
    print("=" * 80)

    device = torch.device("cuda")

    try:
        # åˆ›å»ºæ¨¡å‹å¹¶ç§»åŠ¨åˆ°CUDA
        model = create_hma_unet_local(config="tiny", in_channels=3, num_classes=1).to(device)
        model.eval()

        # æµ‹è¯•è¾“å…¥
        x = torch.randn(1, 3, 128, 128).to(device)
        print(f"Input: {x.shape} on {x.device}")

        # é€æ­¥æµ‹è¯•æ•°æ®æµ
        print("\nğŸ” Testing step-by-step data flow...")

        with torch.no_grad():
            # 1. Stemå±‚
            stem_out = model.encoder.stem(x)
            print(f"   Stem output: {stem_out.shape} on {stem_out.device}")

            # 2. ç¼–ç å™¨å„é˜¶æ®µ
            enc_features = model.encoder(x)
            for i, feat in enumerate(enc_features):
                print(f"   Encoder stage {i+1}: {feat.shape} on {feat.device}")

            # 3. ç“¶é¢ˆå±‚
            bottleneck_out = model.bottleneck(enc_features[-1])
            print(f"   Bottleneck output: {bottleneck_out.shape} on {bottleneck_out.device}")

            # 4. è§£ç å™¨
            decoder_features = [
                enc_features[0],
                enc_features[1],
                enc_features[2],
                stem_out,
            ]
            decoded_out = model.decoder(bottleneck_out, decoder_features)
            print(f"   Decoder output: {decoded_out.shape} on {decoded_out.device}")

            # 5. è¾“å‡ºå¤´
            final_out = model.output_head(decoded_out)
            print(f"   Final output: {final_out.shape} on {final_out.device}")

            # 6. å®Œæ•´å‰å‘ä¼ æ’­
            complete_out = model(x)
            print(f"   Complete forward: {complete_out.shape} on {complete_out.device}")

            # éªŒè¯ä¸€è‡´æ€§
            assert torch.allclose(
                final_out, complete_out, atol=1e-6
            ), "Output mismatch!"

        print("   âœ… Data flow consistency test PASSED")

    except Exception as e:
        print(f"   âŒ Data flow consistency test FAILED: {e}")
        traceback.print_exc()


def test_memory_usage():
    """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ - CUDA only"""
    print("\n" + "=" * 80)
    print("Memory Usage Test (CUDA Required)")
    print("=" * 80)

    device = torch.device("cuda")

    try:
        import psutil
        import os

        process = psutil.Process(os.getpid())

        # ä¸åŒé…ç½®çš„å†…å­˜æµ‹è¯•
        configs = ["tiny"]

        for config in configs:
            print(f"\nğŸ’¾ Testing {config} configuration memory usage...")

            # è®°å½•åˆå§‹å†…å­˜
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB
            initial_gpu_memory = torch.cuda.memory_allocated(device) / 1024 / 1024  # MB

            # åˆ›å»ºæ¨¡å‹å¹¶ç§»åŠ¨åˆ°CUDA
            model = create_hma_unet_local(config=config, in_channels=3, num_classes=1).to(device)
            model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

            # è®°å½•æ¨¡å‹åŠ è½½åå†…å­˜
            model_memory = process.memory_info().rss / 1024 / 1024  # MB
            model_gpu_memory = torch.cuda.memory_allocated(device) / 1024 / 1024  # MB

            # å‰å‘ä¼ æ’­
            x = torch.randn(1, 3, 128, 128).to(device)
            with torch.no_grad():
                output = model(x)

            # è®°å½•å‰å‘ä¼ æ’­åå†…å­˜
            forward_memory = process.memory_info().rss / 1024 / 1024  # MB
            forward_gpu_memory = torch.cuda.memory_allocated(device) / 1024 / 1024  # MB

            print(f"   CPU Memory:")
            print(f"     Initial: {initial_memory:.1f} MB")
            print(f"     After model loading: {model_memory:.1f} MB (+{model_memory-initial_memory:.1f} MB)")
            print(f"     After forward pass: {forward_memory:.1f} MB (+{forward_memory-model_memory:.1f} MB)")
            
            print(f"   GPU Memory:")
            print(f"     Initial: {initial_gpu_memory:.1f} MB")
            print(f"     After model loading: {model_gpu_memory:.1f} MB (+{model_gpu_memory-initial_gpu_memory:.1f} MB)")
            print(f"     After forward pass: {forward_gpu_memory:.1f} MB (+{forward_gpu_memory-model_gpu_memory:.1f} MB)")

            # æ¸…ç†
            del model, x, output
            torch.cuda.empty_cache()

        print("   âœ… Memory usage test PASSED")

    except ImportError:
        print("   âš ï¸  psutil not available, skipping memory test")
    except Exception as e:
        print(f"   âŒ Memory usage test FAILED: {e}")


def main():
    """ä¸»æµ‹è¯•å‡½æ•° - CUDA only"""
    print("ğŸš€ Starting HMA-UNet Model Tests (CUDA Required)...\n")
    
    # å¼ºåˆ¶æ£€æŸ¥CUDAå¯ç”¨æ€§
    check_cuda_availability()
    
    # è®¾ç½®CUDAè®¾å¤‡
    torch.cuda.set_device(0)
    print(f"Using CUDA device: {torch.cuda.current_device()}")

    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    torch.cuda.manual_seed(42)

    # é¦–å…ˆæµ‹è¯•å¯¼å…¥
    if not test_imports():
        print("âŒ Import test failed. Cannot proceed with other tests.")
        return

    # è¿è¡Œå…¶ä»–æµ‹è¯•
    test_basic_components()
    test_encoder_features()
    test_csfg_module()
    test_bottleneck_module()
    test_decoder_module()
    test_model_architecture()
    test_model_gradient_flow()
    test_data_flow_consistency()
    test_memory_usage()

    print("\n" + "=" * 80)
    print("ğŸ‰ HMA-UNet tests completed!")
    print("=" * 80)


if __name__ == "__main__":
    main()
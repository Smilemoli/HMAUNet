import torch
import torch.optim as optim
from torch.utils.data import DataLoader
import argparse
import logging
import os
from datetime import datetime
from tqdm import tqdm

# å¯¼å…¥æ¨¡å—
from models.components.IntegratedDRM import create_integrated_hma_drm
from data.dataset import create_train_val_dataloaders


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='é›†æˆHMA-DRMè®­ç»ƒè„šæœ¬')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--train_img_dir', type=str, required=True, help='è®­ç»ƒå›¾åƒç›®å½•')
    parser.add_argument('--train_mask_dir', type=str, required=True, help='è®­ç»ƒæ©ç ç›®å½•')
    parser.add_argument('--batch_size', type=int, default=4, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--img_size', type=int, default=512, help='å›¾åƒå¤§å°')
    parser.add_argument('--split_ratio', type=float, default=0.8, help='è®­ç»ƒéªŒè¯åˆ†å‰²æ¯”ä¾‹')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--base_channels', type=int, default=32, help='åŸºç¡€é€šé“æ•°')
    parser.add_argument('--timesteps', type=int, default=1000, help='æ‰©æ•£æ—¶é—´æ­¥æ•°')
    parser.add_argument('--diffusion_probability', type=float, default=0.5, help='æ‰©æ•£è®­ç»ƒæ¦‚ç‡')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--num_epochs', type=int, default=100, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=1e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--weight_decay', type=float, default=1e-4, help='æƒé‡è¡°å‡')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--device', type=str, default='cuda', help='è®¡ç®—è®¾å¤‡')
    parser.add_argument('--save_dir', type=str, default='./checkpoints/IntegratedHMADRM', help='ä¿å­˜ç›®å½•')
    parser.add_argument('--val_frequency', type=int, default=5, help='éªŒè¯é¢‘ç‡')
    parser.add_argument('--save_frequency', type=int, default=10, help='ä¿å­˜é¢‘ç‡')
    
    return parser.parse_args()


def setup_logging(save_dir: str):
    """è®¾ç½®æ—¥å¿—"""
    os.makedirs(save_dir, exist_ok=True)
    log_file = os.path.join(save_dir, f'train_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )
    
    return logging.getLogger(__name__)


def compute_dice(pred: torch.Tensor, target: torch.Tensor) -> float:
    """è®¡ç®—Diceç³»æ•°"""
    pred_binary = (pred > 0.5).float()
    intersection = (pred_binary * target).sum()
    dice = (2 * intersection) / (pred_binary.sum() + target.sum() + 1e-8)
    return dice.item()


def compute_iou(pred: torch.Tensor, target: torch.Tensor) -> float:
    """è®¡ç®—IoU"""
    pred_binary = (pred > 0.5).float()
    intersection = (pred_binary * target).sum()
    union = pred_binary.sum() + target.sum() - intersection
    iou = intersection / (union + 1e-8)
    return iou.item()


def train_epoch(model, train_loader, optimizer, device, epoch, logger):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    
    total_loss = 0.0
    total_dice = 0.0
    total_iou = 0.0
    diffusion_count = 0
    direct_count = 0
    num_batches = 0
    
    progress_bar = tqdm(train_loader, desc=f"Epoch {epoch}")
    
    for batch_idx, batch in enumerate(progress_bar):
        try:
            # è·å–æ•°æ®
            if isinstance(batch, dict):
                images = batch['image'].to(device, non_blocking=True)
                masks = batch['mask'].to(device, non_blocking=True)
            else:
                images, masks = batch
                images = images.to(device, non_blocking=True)
                masks = masks.to(device, non_blocking=True)
            
            masks = (masks > 0.5).float()
            
            # å‰å‘ä¼ æ’­
            optimizer.zero_grad()
            loss_dict = model.compute_loss(images, masks)
            
            # åå‘ä¼ æ’­
            loss_dict['total_loss'].backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss_dict['total_loss'].item()
            
            # è®¡ç®—æŒ‡æ ‡ï¼ˆä½¿ç”¨åˆå§‹é¢„æµ‹ï¼‰
            with torch.no_grad():
                outputs = model.forward(images, masks, mode="train")
                pred_masks = outputs['initial_prediction']
                
                dice = compute_dice(pred_masks, masks)
                iou = compute_iou(pred_masks, masks)
                
                total_dice += dice
                total_iou += iou
            
            # ç»Ÿè®¡æ¨¡å¼
            if loss_dict['mode'] == 'diffusion':
                diffusion_count += 1
            else:
                direct_count += 1
            
            num_batches += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'Loss': f"{loss_dict['total_loss'].item():.4f}",
                'Dice': f"{dice:.4f}",
                'Mode': loss_dict['mode'][:4]
            })
            
            # å®šæœŸæ—¥å¿—
            if batch_idx % 50 == 0:
                logger.info(
                    f"Epoch {epoch}, Batch {batch_idx}: "
                    f"Loss={loss_dict['total_loss'].item():.4f}, "
                    f"Mode={loss_dict['mode']}"
                )
                
        except Exception as e:
            logger.error(f"è®­ç»ƒæ‰¹æ¬¡ {batch_idx} å¤±è´¥: {e}")
            continue
    
    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    if num_batches > 0:
        avg_loss = total_loss / num_batches
        avg_dice = total_dice / num_batches
        avg_iou = total_iou / num_batches
    else:
        avg_loss = avg_dice = avg_iou = 0.0
    
    logger.info(f"Epoch {epoch} è®­ç»ƒå®Œæˆ:")
    logger.info(f"  å¹³å‡æŸå¤±: {avg_loss:.4f}")
    logger.info(f"  å¹³å‡Dice: {avg_dice:.4f}")
    logger.info(f"  å¹³å‡IoU: {avg_iou:.4f}")
    logger.info(f"  æ‰©æ•£æ‰¹æ¬¡: {diffusion_count}, ç›´æ¥æ‰¹æ¬¡: {direct_count}")
    
    return {
        'train_loss': avg_loss,
        'train_dice': avg_dice,
        'train_iou': avg_iou,
        'diffusion_ratio': diffusion_count / max(num_batches, 1)
    }


def validate(model, val_loader, device, logger):
    """éªŒè¯"""
    model.eval()
    
    total_dice = 0.0
    total_iou = 0.0
    total_samples = 0
    
    with torch.no_grad():
        for batch in tqdm(val_loader, desc="éªŒè¯ä¸­"):
            try:
                # è·å–æ•°æ®
                if isinstance(batch, dict):
                    images = batch['image'].to(device, non_blocking=True)
                    masks = batch['mask'].to(device, non_blocking=True)
                else:
                    images, masks = batch
                    images = images.to(device, non_blocking=True)
                    masks = masks.to(device, non_blocking=True)
                
                masks = (masks > 0.5).float()
                
                # æ¨ç†
                refined_masks = model.forward(images, mode="inference")
                
                # è®¡ç®—æŒ‡æ ‡
                dice = compute_dice(refined_masks, masks)
                iou = compute_iou(refined_masks, masks)
                
                batch_size = images.shape[0]
                total_dice += dice * batch_size
                total_iou += iou * batch_size
                total_samples += batch_size
                
            except Exception as e:
                logger.error(f"éªŒè¯æ‰¹æ¬¡å¤±è´¥: {e}")
                continue
    
    if total_samples > 0:
        avg_dice = total_dice / total_samples
        avg_iou = total_iou / total_samples
    else:
        avg_dice = avg_iou = 0.0
    
    logger.info(f"éªŒè¯ç»“æœ: Dice={avg_dice:.4f}, IoU={avg_iou:.4f}")
    
    return {
        'val_dice': avg_dice,
        'val_iou': avg_iou
    }


def save_checkpoint(model, optimizer, epoch, metrics, save_dir, is_best=False):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'metrics': metrics,
        'model_config': {
            'base_channels': model.hma_unet.base_channels,
            'timesteps': model.timesteps,
            'use_diffusion_training': model.use_diffusion_training,
            'diffusion_probability': model.diffusion_probability
        }
    }
    
    # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
    latest_path = os.path.join(save_dir, 'latest_checkpoint.pth')
    torch.save(checkpoint, latest_path)
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if is_best:
        best_path = os.path.join(save_dir, 'best_model.pth')
        torch.save(checkpoint, best_path)
        print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {best_path}")
    
    print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {latest_path}")


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    logger = setup_logging(args.save_dir)
    
    # æ‰“å°é…ç½®
    logger.info("=" * 60)
    logger.info("é›†æˆHMA-DRMè®­ç»ƒé…ç½®")
    logger.info("=" * 60)
    logger.info(f"åŸºç¡€é€šé“æ•°: {args.base_channels}")
    logger.info(f"æ‰©æ•£æ—¶é—´æ­¥: {args.timesteps}")
    logger.info(f"æ‰©æ•£æ¦‚ç‡: {args.diffusion_probability}")
    logger.info(f"è®­ç»ƒè½®æ•°: {args.num_epochs}")
    logger.info(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    logger.info(f"å­¦ä¹ ç‡: {args.lr}")
    logger.info(f"è®¾å¤‡: {args.device}")
    logger.info("=" * 60)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, val_loader = create_train_val_dataloaders(
        train_img_dir=args.train_img_dir,
        train_mask_dir=args.train_mask_dir,
        batch_size=args.batch_size,
        img_size=args.img_size,
        split_ratio=args.split_ratio
    )
    
    # åˆ›å»ºæ¨¡å‹
    try:
        model = create_integrated_hma_drm(
            base_channels=args.base_channels,
            timesteps=args.timesteps,
            diffusion_probability=args.diffusion_probability,
            device=args.device
        )
        
        # å†æ¬¡ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        model = model.to(args.device)
        
        # éªŒè¯è®¾å¤‡çŠ¶æ€
        model_device = next(model.parameters()).device
        print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œå½“å‰è®¾å¤‡: {model_device}")
        
        if str(model_device) != args.device:
            logger.warning(f"è®¾å¤‡ä¸åŒ¹é…! é¢„æœŸ: {args.device}, å®é™…: {model_device}")
            model = model.to(args.device)
            print(f"ğŸ”„ å¼ºåˆ¶ç§»åŠ¨åˆ°è®¾å¤‡: {args.device}")
        
    except Exception as e:
        logger.error(f"æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(
        model.parameters(),
        lr=args.lr,
        weight_decay=args.weight_decay
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=args.num_epochs, eta_min=args.lr * 0.01
    )
    
    # è®­ç»ƒå¾ªç¯
    best_dice = 0.0
    
    for epoch in range(1, args.num_epochs + 1):
        try:
            # è®­ç»ƒ
            train_metrics = train_epoch(model, train_loader, optimizer, args.device, epoch, logger)
            scheduler.step()
            
            # éªŒè¯
            if epoch % args.val_frequency == 0:
                val_metrics = validate(model, val_loader, args.device, logger)
                
                # æ£€æŸ¥æœ€ä½³æ¨¡å‹
                current_dice = val_metrics['val_dice']
                is_best = current_dice > best_dice
                if is_best:
                    best_dice = current_dice
                
                # åˆå¹¶æŒ‡æ ‡
                all_metrics = {**train_metrics, **val_metrics}
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                save_checkpoint(model, optimizer, epoch, all_metrics, args.save_dir, is_best)
                
                if is_best:
                    logger.info(f"ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹! Dice: {current_dice:.4f}")
            
            # å®šæœŸä¿å­˜
            elif epoch % args.save_frequency == 0:
                save_checkpoint(model, optimizer, epoch, train_metrics, args.save_dir)
                
        except Exception as e:
            logger.error(f"Epoch {epoch} å¤±è´¥: {e}")
            continue
    
    logger.info(f"âœ… è®­ç»ƒå®Œæˆ! æœ€ä½³Dice: {best_dice:.4f}")


if __name__ == "__main__":
    main()